{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StockStalker.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HN95rcdD0hY"
      },
      "source": [
        "# Estimating whether a stock will go up, down, or hold based on news headlines\n",
        "\n",
        "We will use a Random Forest Classifier, a Naive-Bayes classifier with a \"bag of words\" approach, and an LSTM Recurrent Neural Network to see which gives us the best performance.\n",
        "\n",
        "LABELS: \n",
        "\n",
        "For our initial approaches, prior to trying our neural network's hand at this problem, we'll be using a simpler binary classification, where: \n",
        "\n",
        "0 -> Stocks will go down\n",
        "\n",
        "1 -> Stocks will go up or hold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyr3i6ea8XeI"
      },
      "source": [
        "# Import numpy and pandas to start\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmydtDUxA35g",
        "outputId": "653b8f64-7c7d-4bf6-bb17-8856a44aef9d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJR9Hod7DBqs"
      },
      "source": [
        "# Read in our first dataset as df\n",
        "df = pd.read_csv('/content/drive/MyDrive/StockStalker/Data/Stock_Dataa.csv', encoding='ISO-8859-1')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "mHoB2Yz4DYLU",
        "outputId": "e5305b47-e0d3-4077-b957-f2ff88feeb7d"
      },
      "source": [
        "# Look at first 5 rows\n",
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Label</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2000-01-03</td>\n",
              "      <td>0</td>\n",
              "      <td>A 'hindrance to operations': extracts from the...</td>\n",
              "      <td>Scorecard</td>\n",
              "      <td>Hughes' instant hit buoys Blues</td>\n",
              "      <td>Jack gets his skates on at ice-cold Alex</td>\n",
              "      <td>Chaos as Maracana builds up for United</td>\n",
              "      <td>Depleted Leicester prevail as Elliott spoils E...</td>\n",
              "      <td>Hungry Spurs sense rich pickings</td>\n",
              "      <td>Gunners so wide of an easy target</td>\n",
              "      <td>Derby raise a glass to Strupar's debut double</td>\n",
              "      <td>Southgate strikes, Leeds pay the penalty</td>\n",
              "      <td>Hammers hand Robson a youthful lesson</td>\n",
              "      <td>Saints party like it's 1999</td>\n",
              "      <td>Wear wolves have turned into lambs</td>\n",
              "      <td>Stump mike catches testy Gough's taunt</td>\n",
              "      <td>Langer escapes to hit 167</td>\n",
              "      <td>Flintoff injury piles on woe for England</td>\n",
              "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
              "      <td>Kohl's successor drawn into scandal</td>\n",
              "      <td>The difference between men and women</td>\n",
              "      <td>Sara Denver, nurse turned solicitor</td>\n",
              "      <td>Diana's landmine crusade put Tories in a panic</td>\n",
              "      <td>Yeltsin's resignation caught opposition flat-f...</td>\n",
              "      <td>Russian roulette</td>\n",
              "      <td>Sold out</td>\n",
              "      <td>Recovering a title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2000-01-04</td>\n",
              "      <td>0</td>\n",
              "      <td>Scorecard</td>\n",
              "      <td>The best lake scene</td>\n",
              "      <td>Leader: German sleaze inquiry</td>\n",
              "      <td>Cheerio, boyo</td>\n",
              "      <td>The main recommendations</td>\n",
              "      <td>Has Cubie killed fees?</td>\n",
              "      <td>Has Cubie killed fees?</td>\n",
              "      <td>Has Cubie killed fees?</td>\n",
              "      <td>Hopkins 'furious' at Foster's lack of Hannibal...</td>\n",
              "      <td>Has Cubie killed fees?</td>\n",
              "      <td>A tale of two tails</td>\n",
              "      <td>I say what I like and I like what I say</td>\n",
              "      <td>Elbows, Eyes and Nipples</td>\n",
              "      <td>Task force to assess risk of asteroid collision</td>\n",
              "      <td>How I found myself at last</td>\n",
              "      <td>On the critical list</td>\n",
              "      <td>The timing of their lives</td>\n",
              "      <td>Dear doctor</td>\n",
              "      <td>Irish court halts IRA man's extradition to Nor...</td>\n",
              "      <td>Burundi peace initiative fades after rebels re...</td>\n",
              "      <td>PE points the way forward to the ECB</td>\n",
              "      <td>Campaigners keep up pressure on Nazi war crime...</td>\n",
              "      <td>Jane Ratcliffe</td>\n",
              "      <td>Yet more things you wouldn't know without the ...</td>\n",
              "      <td>Millennium bug fails to bite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2000-01-05</td>\n",
              "      <td>0</td>\n",
              "      <td>Coventry caught on counter by Flo</td>\n",
              "      <td>United's rivals on the road to Rio</td>\n",
              "      <td>Thatcher issues defence before trial by video</td>\n",
              "      <td>Police help Smith lay down the law at Everton</td>\n",
              "      <td>Tale of Trautmann bears two more retellings</td>\n",
              "      <td>England on the rack</td>\n",
              "      <td>Pakistan retaliate with call for video of Walsh</td>\n",
              "      <td>Cullinan continues his Cape monopoly</td>\n",
              "      <td>McGrath puts India out of their misery</td>\n",
              "      <td>Blair Witch bandwagon rolls on</td>\n",
              "      <td>Pele turns up heat on Ferguson</td>\n",
              "      <td>Party divided over Kohl slush fund scandal</td>\n",
              "      <td>Manchester United (England)</td>\n",
              "      <td>Women in record South Pole walk</td>\n",
              "      <td>Vasco da Gama (Brazil)</td>\n",
              "      <td>South Melbourne (Australia)</td>\n",
              "      <td>Necaxa (Mexico)</td>\n",
              "      <td>Real Madrid (Spain)</td>\n",
              "      <td>Raja Casablanca (Morocco)</td>\n",
              "      <td>Corinthians (Brazil)</td>\n",
              "      <td>Tony's pet project</td>\n",
              "      <td>Al Nassr (Saudi Arabia)</td>\n",
              "      <td>Ideal Holmes show</td>\n",
              "      <td>Pinochet leaves hospital after tests</td>\n",
              "      <td>Useful links</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2000-01-06</td>\n",
              "      <td>1</td>\n",
              "      <td>Pilgrim knows how to progress</td>\n",
              "      <td>Thatcher facing ban</td>\n",
              "      <td>McIlroy calls for Irish fighting spirit</td>\n",
              "      <td>Leicester bin stadium blueprint</td>\n",
              "      <td>United braced for Mexican wave</td>\n",
              "      <td>Auntie back in fashion, even if the dress look...</td>\n",
              "      <td>Shoaib appeal goes to the top</td>\n",
              "      <td>Hussain hurt by 'shambles' but lays blame on e...</td>\n",
              "      <td>England's decade of disasters</td>\n",
              "      <td>Revenge is sweet for jubilant Cronje</td>\n",
              "      <td>Our choice, not theirs</td>\n",
              "      <td>Profile of former US Nazi Party officer Willia...</td>\n",
              "      <td>New evidence shows record of war crimes suspec...</td>\n",
              "      <td>The rise of the supernerds</td>\n",
              "      <td>Written on the body</td>\n",
              "      <td>Putin admits Yeltsin quit to give him a head s...</td>\n",
              "      <td>BBC worst hit as digital TV begins to bite</td>\n",
              "      <td>How much can you pay for...</td>\n",
              "      <td>Christmas glitches</td>\n",
              "      <td>Upending a table, Chopping a line and Scoring ...</td>\n",
              "      <td>Scientific evidence 'unreliable', defence claims</td>\n",
              "      <td>Fusco wins judicial review in extradition case</td>\n",
              "      <td>Rebels thwart Russian advance</td>\n",
              "      <td>Blair orders shake-up of failing NHS</td>\n",
              "      <td>Lessons of law's hard heart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2000-01-07</td>\n",
              "      <td>1</td>\n",
              "      <td>Hitches and Horlocks</td>\n",
              "      <td>Beckham off but United survive</td>\n",
              "      <td>Breast cancer screening</td>\n",
              "      <td>Alan Parker</td>\n",
              "      <td>Guardian readers: are you all whingers?</td>\n",
              "      <td>Hollywood Beyond</td>\n",
              "      <td>Ashes and diamonds</td>\n",
              "      <td>Whingers - a formidable minority</td>\n",
              "      <td>Alan Parker - part two</td>\n",
              "      <td>Thuggery, Toxins and Ties</td>\n",
              "      <td>Met faces fresh attack on race crime</td>\n",
              "      <td>Everton fans top racist 'league of shame'</td>\n",
              "      <td>Our breasts, ourselves</td>\n",
              "      <td>Russia's new boss has an extremely strange his...</td>\n",
              "      <td>Always and forever</td>\n",
              "      <td>Most everywhere:  UDIs</td>\n",
              "      <td>Most wanted:  Chloe lunettes</td>\n",
              "      <td>Return of the cane 'completely off the agenda'</td>\n",
              "      <td>From Sleepy Hollow to Greeneland</td>\n",
              "      <td>Blunkett outlines vision for over 11s</td>\n",
              "      <td>Embattled Dobson attacks 'play now, pay later'...</td>\n",
              "      <td>Doom and the Dome</td>\n",
              "      <td>What is the north-south divide?</td>\n",
              "      <td>Aitken released from jail</td>\n",
              "      <td>Gone aloft</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date  ...                         Top25\n",
              "0  2000-01-03  ...            Recovering a title\n",
              "1  2000-01-04  ...  Millennium bug fails to bite\n",
              "2  2000-01-05  ...                  Useful links\n",
              "3  2000-01-06  ...   Lessons of law's hard heart\n",
              "4  2000-01-07  ...                    Gone aloft\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "FCRU6IjNDaw8",
        "outputId": "03451653-71a4-483d-e77b-4473d89efbec"
      },
      "source": [
        "# Look at last 5 rows\n",
        "df.tail()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Label</th>\n",
              "      <th>Top1</th>\n",
              "      <th>Top2</th>\n",
              "      <th>Top3</th>\n",
              "      <th>Top4</th>\n",
              "      <th>Top5</th>\n",
              "      <th>Top6</th>\n",
              "      <th>Top7</th>\n",
              "      <th>Top8</th>\n",
              "      <th>Top9</th>\n",
              "      <th>Top10</th>\n",
              "      <th>Top11</th>\n",
              "      <th>Top12</th>\n",
              "      <th>Top13</th>\n",
              "      <th>Top14</th>\n",
              "      <th>Top15</th>\n",
              "      <th>Top16</th>\n",
              "      <th>Top17</th>\n",
              "      <th>Top18</th>\n",
              "      <th>Top19</th>\n",
              "      <th>Top20</th>\n",
              "      <th>Top21</th>\n",
              "      <th>Top22</th>\n",
              "      <th>Top23</th>\n",
              "      <th>Top24</th>\n",
              "      <th>Top25</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4096</th>\n",
              "      <td>2016-06-27</td>\n",
              "      <td>0</td>\n",
              "      <td>Barclays and RBS shares suspended from trading...</td>\n",
              "      <td>Pope says Church should ask forgiveness from g...</td>\n",
              "      <td>Poland 'shocked' by xenophobic abuse of Poles ...</td>\n",
              "      <td>There will be no second referendum, cabinet ag...</td>\n",
              "      <td>Scotland welcome to join EU, Merkel ally says</td>\n",
              "      <td>Sterling dips below Friday's 31-year low amid ...</td>\n",
              "      <td>No negative news about South African President...</td>\n",
              "      <td>Surge in Hate Crimes in the U.K. Following U.K...</td>\n",
              "      <td>Weapons shipped into Jordan by the CIA and Sau...</td>\n",
              "      <td>Angela Merkel said the U.K. must file exit pap...</td>\n",
              "      <td>In a birth offering hope to a threatened speci...</td>\n",
              "      <td>Sky News Journalist Left Speechless As Leave M...</td>\n",
              "      <td>Giant panda in Macau gives birth to twins</td>\n",
              "      <td>Get out now: EU leader tells Britain it must i...</td>\n",
              "      <td>Sea turtle 'beaten and left for dead' on beach...</td>\n",
              "      <td>German lawyers to probe Erdogan over alleged w...</td>\n",
              "      <td>Boris Johnson says the UK will continue to \"in...</td>\n",
              "      <td>Richard Branson is calling on the UK governmen...</td>\n",
              "      <td>Turkey 'sorry for downing Russian jet'</td>\n",
              "      <td>Edward Snowden lawyer vows new push for pardon...</td>\n",
              "      <td>Brexit opinion poll reveals majority don't wan...</td>\n",
              "      <td>Conservative MP Leave Campaigner: \"The leave c...</td>\n",
              "      <td>Economists predict UK recession, further weake...</td>\n",
              "      <td>New EU 'superstate plan by France, Germany: Cr...</td>\n",
              "      <td>Pakistani clerics declare transgender marriage...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4097</th>\n",
              "      <td>2016-06-28</td>\n",
              "      <td>1</td>\n",
              "      <td>2,500 Scientists To Australia: If You Want To ...</td>\n",
              "      <td>The personal details of 112,000 French police ...</td>\n",
              "      <td>S&amp;amp;P cuts United Kingdom sovereign credit r...</td>\n",
              "      <td>Huge helium deposit found in Africa</td>\n",
              "      <td>CEO of the South African state broadcaster qui...</td>\n",
              "      <td>Brexit cost investors $2 trillion, the worst o...</td>\n",
              "      <td>Hong Kong democracy activists call for return ...</td>\n",
              "      <td>Brexit: Iceland president says UK can join 'tr...</td>\n",
              "      <td>UK's Osborne: 'Absolutely' going to have to cu...</td>\n",
              "      <td>'Do not let Scotland down now' : Scottish MEP ...</td>\n",
              "      <td>British pound could hit history-making dollar ...</td>\n",
              "      <td>Merkel vows to strengthen EU, tells UK no 'che...</td>\n",
              "      <td>\"Ryanair will not deploy new aircraft on route...</td>\n",
              "      <td>People, ever more greedy and stupid, destroy t...</td>\n",
              "      <td>Siemens freezes new UK wind power investment f...</td>\n",
              "      <td>US, Canada and Mexico pledge 50% of power from...</td>\n",
              "      <td>There is increasing evidence that Australia is...</td>\n",
              "      <td>Richard Branson, the founder of Virgin Group, ...</td>\n",
              "      <td>37,000-yr-old skull from Borneo reveals surpri...</td>\n",
              "      <td>Palestinians stone Western Wall worshipers; po...</td>\n",
              "      <td>Jean-Claude Juncker asks Farage: Why are you h...</td>\n",
              "      <td>\"Romanians for Remainians\" offering a new home...</td>\n",
              "      <td>Brexit: Gibraltar in talks with Scotland to st...</td>\n",
              "      <td>8 Suicide Bombers Strike Lebanon</td>\n",
              "      <td>Mexico's security forces routinely use 'sexual...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4098</th>\n",
              "      <td>2016-06-29</td>\n",
              "      <td>1</td>\n",
              "      <td>Explosion At Airport In Istanbul</td>\n",
              "      <td>Yemeni former president: Terrorism is the offs...</td>\n",
              "      <td>UK must accept freedom of movement to access E...</td>\n",
              "      <td>Devastated: scientists too late to captive bre...</td>\n",
              "      <td>British Labor Party leader Jeremy Corbyn loses...</td>\n",
              "      <td>A Muslim Shop in the UK Was Just Firebombed Wh...</td>\n",
              "      <td>Mexican Authorities Sexually Torture Women in ...</td>\n",
              "      <td>UK shares and pound continue to recover</td>\n",
              "      <td>Iceland historian Johannesson wins presidentia...</td>\n",
              "      <td>99-Million-Yr-Old Bird Wings Found Encased in ...</td>\n",
              "      <td>A chatbot programmed by a British teenager has...</td>\n",
              "      <td>The Philippine president-elect said Monday he ...</td>\n",
              "      <td>Former Belgian Prime Minister ridicules Nigel ...</td>\n",
              "      <td>Brexiteer Nigel Farage To EU: 'You're Not Laug...</td>\n",
              "      <td>Islamic State bombings in southern Yemen kill ...</td>\n",
              "      <td>Escape Tunnel, Dug by Hand, Is Found at Holoca...</td>\n",
              "      <td>The land under Beijing is sinking by as much a...</td>\n",
              "      <td>Car bomb and Anti-Islamic attack on Mosque in ...</td>\n",
              "      <td>Emaciated lions in Taiz Zoo are trapped in blo...</td>\n",
              "      <td>Rupert Murdoch describes Brexit as 'wonderful'...</td>\n",
              "      <td>More than 40 killed in Yemen suicide attacks</td>\n",
              "      <td>Google Found Disastrous Symantec and Norton Vu...</td>\n",
              "      <td>Extremist violence on the rise in Germany: Dom...</td>\n",
              "      <td>BBC News: Labour MPs pass Corbyn no-confidence...</td>\n",
              "      <td>Tiny New Zealand town with 'too many jobs' lau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4099</th>\n",
              "      <td>2016-06-30</td>\n",
              "      <td>1</td>\n",
              "      <td>Jamaica proposes marijuana dispensers for tour...</td>\n",
              "      <td>Stephen Hawking says pollution and 'stupidity'...</td>\n",
              "      <td>Boris Johnson says he will not run for Tory pa...</td>\n",
              "      <td>Six gay men in Ivory Coast were abused and for...</td>\n",
              "      <td>Switzerland denies citizenship to Muslim immig...</td>\n",
              "      <td>Palestinian terrorist stabs israeli teen girl ...</td>\n",
              "      <td>Puerto Rico will default on $1 billion of debt...</td>\n",
              "      <td>Republic of Ireland fans to be awarded medal f...</td>\n",
              "      <td>Afghan suicide bomber 'kills up to 40' - BBC News</td>\n",
              "      <td>US airstrikes kill at least 250 ISIS fighters ...</td>\n",
              "      <td>Turkish Cop Who Took Down Istanbul Gunman Hail...</td>\n",
              "      <td>Cannabis compounds could treat Alzheimer's by ...</td>\n",
              "      <td>Japan's top court has approved blanket surveil...</td>\n",
              "      <td>CIA Gave Romania Millions to Host Secret Prisons</td>\n",
              "      <td>Groups urge U.N. to suspend Saudi Arabia from ...</td>\n",
              "      <td>Googles free wifi at Indian railway stations i...</td>\n",
              "      <td>Mounting evidence suggests 'hobbits' were wipe...</td>\n",
              "      <td>The men who carried out Tuesday's terror attac...</td>\n",
              "      <td>Calls to suspend Saudi Arabia from UN Human Ri...</td>\n",
              "      <td>More Than 100 Nobel Laureates Call Out Greenpe...</td>\n",
              "      <td>British pedophile sentenced to 85 years in US ...</td>\n",
              "      <td>US permitted 1,200 offshore fracks in Gulf of ...</td>\n",
              "      <td>We will be swimming in ridicule - French beach...</td>\n",
              "      <td>UEFA says no minutes of silence for Istanbul v...</td>\n",
              "      <td>Law Enforcement Sources: Gun Used in Paris Ter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4100</th>\n",
              "      <td>2016-07-01</td>\n",
              "      <td>1</td>\n",
              "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
              "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
              "      <td>The president of France says if Brexit won, so...</td>\n",
              "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
              "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
              "      <td>Brazil: Huge spike in number of police killing...</td>\n",
              "      <td>Austria's highest court annuls presidential el...</td>\n",
              "      <td>Facebook wins privacy case, can track any Belg...</td>\n",
              "      <td>Switzerland denies Muslim girls citizenship af...</td>\n",
              "      <td>China kills millions of innocent meditators fo...</td>\n",
              "      <td>France Cracks Down on Factory Farms - A viral ...</td>\n",
              "      <td>Abbas PLO Faction Calls Killer of 13-Year-Old ...</td>\n",
              "      <td>Taiwanese warship accidentally fires missile t...</td>\n",
              "      <td>Iran celebrates American Human Rights Week, mo...</td>\n",
              "      <td>U.N. panel moves to curb bias against L.G.B.T....</td>\n",
              "      <td>The United States has placed Myanmar, Uzbekist...</td>\n",
              "      <td>S&amp;amp;P revises European Union credit rating t...</td>\n",
              "      <td>India gets $1 billion loan from World Bank for...</td>\n",
              "      <td>U.S. sailors detained by Iran spoke too much u...</td>\n",
              "      <td>Mass fish kill in Vietnam solved as Taiwan ste...</td>\n",
              "      <td>Philippines president Rodrigo Duterte urges pe...</td>\n",
              "      <td>Spain arrests three Pakistanis accused of prom...</td>\n",
              "      <td>Venezuela, where anger over food shortages is ...</td>\n",
              "      <td>A Hindu temple worker has been killed by three...</td>\n",
              "      <td>Ozone layer hole seems to be healing - US &amp;amp...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Date  ...                                              Top25\n",
              "4096  2016-06-27  ...  Pakistani clerics declare transgender marriage...\n",
              "4097  2016-06-28  ...  Mexico's security forces routinely use 'sexual...\n",
              "4098  2016-06-29  ...  Tiny New Zealand town with 'too many jobs' lau...\n",
              "4099  2016-06-30  ...  Law Enforcement Sources: Gun Used in Paris Ter...\n",
              "4100  2016-07-01  ...  Ozone layer hole seems to be healing - US &amp...\n",
              "\n",
              "[5 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh9bfHATEo6g"
      },
      "source": [
        "Data preprocessing for our text data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "id": "aegGn6IAEr5I",
        "outputId": "b1e0da94-22a6-497b-a079-bf8ea885a1ea"
      },
      "source": [
        "# Remove punctuation\n",
        "data = df.iloc[:,2:27]\n",
        "data.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n",
        "\n",
        "# Also, I'm going to rename our columns for ease of access\n",
        "list1 = [i for i in range(25)]\n",
        "new_index = [str(i) for i in list1]\n",
        "data.columns = new_index\n",
        "data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A  hindrance to operations   extracts from the...</td>\n",
              "      <td>Scorecard</td>\n",
              "      <td>Hughes  instant hit buoys Blues</td>\n",
              "      <td>Jack gets his skates on at ice cold Alex</td>\n",
              "      <td>Chaos as Maracana builds up for United</td>\n",
              "      <td>Depleted Leicester prevail as Elliott spoils E...</td>\n",
              "      <td>Hungry Spurs sense rich pickings</td>\n",
              "      <td>Gunners so wide of an easy target</td>\n",
              "      <td>Derby raise a glass to Strupar s debut double</td>\n",
              "      <td>Southgate strikes  Leeds pay the penalty</td>\n",
              "      <td>Hammers hand Robson a youthful lesson</td>\n",
              "      <td>Saints party like it s</td>\n",
              "      <td>Wear wolves have turned into lambs</td>\n",
              "      <td>Stump mike catches testy Gough s taunt</td>\n",
              "      <td>Langer escapes to hit</td>\n",
              "      <td>Flintoff injury piles on woe for England</td>\n",
              "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
              "      <td>Kohl s successor drawn into scandal</td>\n",
              "      <td>The difference between men and women</td>\n",
              "      <td>Sara Denver  nurse turned solicitor</td>\n",
              "      <td>Diana s landmine crusade put Tories in a panic</td>\n",
              "      <td>Yeltsin s resignation caught opposition flat f...</td>\n",
              "      <td>Russian roulette</td>\n",
              "      <td>Sold out</td>\n",
              "      <td>Recovering a title</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Scorecard</td>\n",
              "      <td>The best lake scene</td>\n",
              "      <td>Leader  German sleaze inquiry</td>\n",
              "      <td>Cheerio  boyo</td>\n",
              "      <td>The main recommendations</td>\n",
              "      <td>Has Cubie killed fees</td>\n",
              "      <td>Has Cubie killed fees</td>\n",
              "      <td>Has Cubie killed fees</td>\n",
              "      <td>Hopkins  furious  at Foster s lack of Hannibal...</td>\n",
              "      <td>Has Cubie killed fees</td>\n",
              "      <td>A tale of two tails</td>\n",
              "      <td>I say what I like and I like what I say</td>\n",
              "      <td>Elbows  Eyes and Nipples</td>\n",
              "      <td>Task force to assess risk of asteroid collision</td>\n",
              "      <td>How I found myself at last</td>\n",
              "      <td>On the critical list</td>\n",
              "      <td>The timing of their lives</td>\n",
              "      <td>Dear doctor</td>\n",
              "      <td>Irish court halts IRA man s extradition to Nor...</td>\n",
              "      <td>Burundi peace initiative fades after rebels re...</td>\n",
              "      <td>PE points the way forward to the ECB</td>\n",
              "      <td>Campaigners keep up pressure on Nazi war crime...</td>\n",
              "      <td>Jane Ratcliffe</td>\n",
              "      <td>Yet more things you wouldn t know without the ...</td>\n",
              "      <td>Millennium bug fails to bite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Coventry caught on counter by Flo</td>\n",
              "      <td>United s rivals on the road to Rio</td>\n",
              "      <td>Thatcher issues defence before trial by video</td>\n",
              "      <td>Police help Smith lay down the law at Everton</td>\n",
              "      <td>Tale of Trautmann bears two more retellings</td>\n",
              "      <td>England on the rack</td>\n",
              "      <td>Pakistan retaliate with call for video of Walsh</td>\n",
              "      <td>Cullinan continues his Cape monopoly</td>\n",
              "      <td>McGrath puts India out of their misery</td>\n",
              "      <td>Blair Witch bandwagon rolls on</td>\n",
              "      <td>Pele turns up heat on Ferguson</td>\n",
              "      <td>Party divided over Kohl slush fund scandal</td>\n",
              "      <td>Manchester United  England</td>\n",
              "      <td>Women in record South Pole walk</td>\n",
              "      <td>Vasco da Gama  Brazil</td>\n",
              "      <td>South Melbourne  Australia</td>\n",
              "      <td>Necaxa  Mexico</td>\n",
              "      <td>Real Madrid  Spain</td>\n",
              "      <td>Raja Casablanca  Morocco</td>\n",
              "      <td>Corinthians  Brazil</td>\n",
              "      <td>Tony s pet project</td>\n",
              "      <td>Al Nassr  Saudi Arabia</td>\n",
              "      <td>Ideal Holmes show</td>\n",
              "      <td>Pinochet leaves hospital after tests</td>\n",
              "      <td>Useful links</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pilgrim knows how to progress</td>\n",
              "      <td>Thatcher facing ban</td>\n",
              "      <td>McIlroy calls for Irish fighting spirit</td>\n",
              "      <td>Leicester bin stadium blueprint</td>\n",
              "      <td>United braced for Mexican wave</td>\n",
              "      <td>Auntie back in fashion  even if the dress look...</td>\n",
              "      <td>Shoaib appeal goes to the top</td>\n",
              "      <td>Hussain hurt by  shambles  but lays blame on e...</td>\n",
              "      <td>England s decade of disasters</td>\n",
              "      <td>Revenge is sweet for jubilant Cronje</td>\n",
              "      <td>Our choice  not theirs</td>\n",
              "      <td>Profile of former US Nazi Party officer Willia...</td>\n",
              "      <td>New evidence shows record of war crimes suspec...</td>\n",
              "      <td>The rise of the supernerds</td>\n",
              "      <td>Written on the body</td>\n",
              "      <td>Putin admits Yeltsin quit to give him a head s...</td>\n",
              "      <td>BBC worst hit as digital TV begins to bite</td>\n",
              "      <td>How much can you pay for</td>\n",
              "      <td>Christmas glitches</td>\n",
              "      <td>Upending a table  Chopping a line and Scoring ...</td>\n",
              "      <td>Scientific evidence  unreliable   defence claims</td>\n",
              "      <td>Fusco wins judicial review in extradition case</td>\n",
              "      <td>Rebels thwart Russian advance</td>\n",
              "      <td>Blair orders shake up of failing NHS</td>\n",
              "      <td>Lessons of law s hard heart</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Hitches and Horlocks</td>\n",
              "      <td>Beckham off but United survive</td>\n",
              "      <td>Breast cancer screening</td>\n",
              "      <td>Alan Parker</td>\n",
              "      <td>Guardian readers  are you all whingers</td>\n",
              "      <td>Hollywood Beyond</td>\n",
              "      <td>Ashes and diamonds</td>\n",
              "      <td>Whingers   a formidable minority</td>\n",
              "      <td>Alan Parker   part two</td>\n",
              "      <td>Thuggery  Toxins and Ties</td>\n",
              "      <td>Met faces fresh attack on race crime</td>\n",
              "      <td>Everton fans top racist  league of shame</td>\n",
              "      <td>Our breasts  ourselves</td>\n",
              "      <td>Russia s new boss has an extremely strange his...</td>\n",
              "      <td>Always and forever</td>\n",
              "      <td>Most everywhere   UDIs</td>\n",
              "      <td>Most wanted   Chloe lunettes</td>\n",
              "      <td>Return of the cane  completely off the agenda</td>\n",
              "      <td>From Sleepy Hollow to Greeneland</td>\n",
              "      <td>Blunkett outlines vision for over   s</td>\n",
              "      <td>Embattled Dobson attacks  play now  pay later ...</td>\n",
              "      <td>Doom and the Dome</td>\n",
              "      <td>What is the north south divide</td>\n",
              "      <td>Aitken released from jail</td>\n",
              "      <td>Gone aloft</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  ...                            24\n",
              "0  A  hindrance to operations   extracts from the...  ...            Recovering a title\n",
              "1                                          Scorecard  ...  Millennium bug fails to bite\n",
              "2                  Coventry caught on counter by Flo  ...                  Useful links\n",
              "3                      Pilgrim knows how to progress  ...   Lessons of law s hard heart\n",
              "4                               Hitches and Horlocks  ...                    Gone aloft\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "Zh7NJJudFAQQ",
        "outputId": "abb5d59a-31f1-4e79-f76e-ed103ae15219"
      },
      "source": [
        "# Converting the headlines into lowercase\n",
        "# so all words are treated the same (apple and Apple, for example)\n",
        "for index in new_index:\n",
        "  data[index] = data[index].str.lower()\n",
        "data.head(1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a  hindrance to operations   extracts from the...</td>\n",
              "      <td>scorecard</td>\n",
              "      <td>hughes  instant hit buoys blues</td>\n",
              "      <td>jack gets his skates on at ice cold alex</td>\n",
              "      <td>chaos as maracana builds up for united</td>\n",
              "      <td>depleted leicester prevail as elliott spoils e...</td>\n",
              "      <td>hungry spurs sense rich pickings</td>\n",
              "      <td>gunners so wide of an easy target</td>\n",
              "      <td>derby raise a glass to strupar s debut double</td>\n",
              "      <td>southgate strikes  leeds pay the penalty</td>\n",
              "      <td>hammers hand robson a youthful lesson</td>\n",
              "      <td>saints party like it s</td>\n",
              "      <td>wear wolves have turned into lambs</td>\n",
              "      <td>stump mike catches testy gough s taunt</td>\n",
              "      <td>langer escapes to hit</td>\n",
              "      <td>flintoff injury piles on woe for england</td>\n",
              "      <td>hunters threaten jospin with new battle of the...</td>\n",
              "      <td>kohl s successor drawn into scandal</td>\n",
              "      <td>the difference between men and women</td>\n",
              "      <td>sara denver  nurse turned solicitor</td>\n",
              "      <td>diana s landmine crusade put tories in a panic</td>\n",
              "      <td>yeltsin s resignation caught opposition flat f...</td>\n",
              "      <td>russian roulette</td>\n",
              "      <td>sold out</td>\n",
              "      <td>recovering a title</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0  ...                  24\n",
              "0  a  hindrance to operations   extracts from the...  ...  recovering a title\n",
              "\n",
              "[1 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "0PCfedbvGdP0",
        "outputId": "d11fcaf6-e518-444f-c08e-5e411d0a0387"
      },
      "source": [
        "# Combining the first 25 headlines of first record\n",
        "' '.join(str(x) for x in data.iloc[1,0:25])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'scorecard the best lake scene leader  german sleaze inquiry cheerio  boyo the main recommendations has cubie killed fees  has cubie killed fees  has cubie killed fees  hopkins  furious  at foster s lack of hannibal appetite has cubie killed fees  a tale of two tails i say what i like and i like what i say elbows  eyes and nipples task force to assess risk of asteroid collision how i found myself at last on the critical list the timing of their lives dear doctor irish court halts ira man s extradition to northern ireland burundi peace initiative fades after rebels reject mandela as mediator pe points the way forward to the ecb campaigners keep up pressure on nazi war crimes suspect jane ratcliffe yet more things you wouldn t know without the movies millennium bug fails to bite'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ro3F-DaGn6o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "3253d34c-3941-4ead-d3fa-8b6dbf239a91"
      },
      "source": [
        "# Combining the headlines for the rest of our records so we can convert them into vectors\n",
        "headlines = []\n",
        "for row in range(0, len(data.index)):\n",
        "  headlines.append(' '.join(str(x) for x in data.iloc[row,0:25]))\n",
        "\n",
        "\n",
        "df['Headlines'] = headlines\n",
        "\n",
        "data = df[['Label', 'Headlines']]\n",
        "data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Headlines</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>a  hindrance to operations   extracts from the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>scorecard the best lake scene leader  german s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>coventry caught on counter by flo united s riv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>pilgrim knows how to progress thatcher facing ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>hitches and horlocks beckham off but united su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4096</th>\n",
              "      <td>0</td>\n",
              "      <td>barclays and rbs shares suspended from trading...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4097</th>\n",
              "      <td>1</td>\n",
              "      <td>scientists to australia  if you want to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4098</th>\n",
              "      <td>1</td>\n",
              "      <td>explosion at airport in istanbul yemeni former...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4099</th>\n",
              "      <td>1</td>\n",
              "      <td>jamaica proposes marijuana dispensers for tour...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4100</th>\n",
              "      <td>1</td>\n",
              "      <td>a     year old woman in mexico city finally re...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4101 rows  2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Label                                          Headlines\n",
              "0         0  a  hindrance to operations   extracts from the...\n",
              "1         0  scorecard the best lake scene leader  german s...\n",
              "2         0  coventry caught on counter by flo united s riv...\n",
              "3         1  pilgrim knows how to progress thatcher facing ...\n",
              "4         1  hitches and horlocks beckham off but united su...\n",
              "...     ...                                                ...\n",
              "4096      0  barclays and rbs shares suspended from trading...\n",
              "4097      1        scientists to australia  if you want to ...\n",
              "4098      1  explosion at airport in istanbul yemeni former...\n",
              "4099      1  jamaica proposes marijuana dispensers for tour...\n",
              "4100      1  a     year old woman in mexico city finally re...\n",
              "\n",
              "[4101 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSk4ZqWHDik8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a26c270-6066-4a32-97c4-47b294a0b016"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set our X and y\n",
        "# X -> features\n",
        "# y -> target\n",
        "\n",
        "X = data['Headlines']\n",
        "y = data[\"Label\"]\n",
        "\n",
        "print(len(data))\n",
        "print(len(y))\n",
        "\n",
        "# Perform our train test split using sklearn's train_test_split function\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(y_train))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4101\n",
            "4101\n",
            "3075\n",
            "3075\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFut9la6G2zw"
      },
      "source": [
        "## Natural Language Processing\n",
        "\n",
        "Using 'bag of words' model and a TF-IDF Vectorizer for converting text data into vectors. \n",
        "\n",
        "For both Random Forest Classifier & Naive-Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I105beVWqPQw"
      },
      "source": [
        "# Count vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7nxIGEZqaJo"
      },
      "source": [
        "# Implementing Bag of Words model: \n",
        "countvector = CountVectorizer(ngram_range=(2, 2))\n",
        "train_dataset = countvector.fit_transform(X_train) # Converting all the headlines to vectors"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grL_ZJI9qsgo",
        "outputId": "d13e487c-f460-4455-c759-662a3c62e8c6"
      },
      "source": [
        "# Data converts into sparse matrix \n",
        "train_dataset[0]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x477881 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 416 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyCfiHO4qx-o"
      },
      "source": [
        "First, we'll use our bag of words model with Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSa8_-KLq0is",
        "outputId": "37221084-e307-4687-b677-dd2360fc63b1"
      },
      "source": [
        "# Implement random forest classifier\n",
        "random_classifier = RandomForestClassifier(n_estimators=200, criterion='entropy')\n",
        "random_classifier.fit(train_dataset, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35I69E0IzWnI"
      },
      "source": [
        "# Predicting for our test dataset\n",
        "test_dataset = countvector.transform(X_test)\n",
        "predictions= random_classifier.predict(test_dataset)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_qxJGm8z9rI",
        "outputId": "2d2dfb70-e72b-4bef-fed4-efd9805f2dab"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, ..., 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D4CaOmN0jZM"
      },
      "source": [
        "# For calculating our accuracy\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpS_3gRb1fsk",
        "outputId": "a2b3855e-1928-4558-a296-fd0f3feb006d"
      },
      "source": [
        "matrix= confusion_matrix(y_test,predictions)\n",
        "print(matrix)\n",
        "score= accuracy_score(y_test,predictions)\n",
        "print(score)\n",
        "report= classification_report(y_test,predictions)\n",
        "print(report)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 34 458]\n",
            " [ 33 501]]\n",
            "0.5214424951267057\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.07      0.12       492\n",
            "           1       0.52      0.94      0.67       534\n",
            "\n",
            "    accuracy                           0.52      1026\n",
            "   macro avg       0.51      0.50      0.40      1026\n",
            "weighted avg       0.52      0.52      0.41      1026\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IzMZISl195Y"
      },
      "source": [
        "It looks like our overall accuracy is around 51% with this approach. \n",
        "\n",
        "The notebook I based my initial preprocessing and approach off (cited below, Sun, J.) acheived a nearly 84% accuracy with this approach, although I've split my test and train data somewhat differently. \n",
        "\n",
        "In Sun, J.'s approach, the test and train data actually overlapped quite a bit: for the entire year 2015. My concern is with having a model overfit in our production case, and so I opted to use the SciKit Learn's built in train_test_split function. \n",
        "\n",
        "In the future, after experimenting with other models and architectures, I will come back and potentially explore other ways of optimizing these results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQRb8-rc2ha0"
      },
      "source": [
        "# Using Random Forest Classfier with TF-IDF Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWU73bwm2gIA"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcFEBOpf2yp8"
      },
      "source": [
        "tfidf = TfidfVectorizer(ngram_range=(2, 2))\n",
        "train_dataset= tfidf.fit_transform(X_train) # Changing all our headlines to vectors using TF-IDF technique"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqQembFA3H64",
        "outputId": "443d659e-5299-47ca-9998-5a27ca49a9aa"
      },
      "source": [
        "# Implement random forest classfier on train_dataset\n",
        "random_classifier = RandomForestClassifier(n_estimators=200, criterion='entropy')\n",
        "random_classifier.fit(train_dataset, y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "welCIZCI3grQ"
      },
      "source": [
        "# Predict on test dataset\n",
        "test_dataset= tfidf.transform(X_test)\n",
        "predictions= random_classifier.predict(test_dataset)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwk8-fTW3-Sk",
        "outputId": "a284d6a5-aadb-420a-cea7-1065609927bc"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQg7A0dQ4BqM",
        "outputId": "676ee85a-06d6-440b-cac4-c45e7abcd902"
      },
      "source": [
        "# ACCURACY AFTER USING TF-IDF VECTORIZER\n",
        "matrix= confusion_matrix(y_test,predictions)\n",
        "print(matrix)\n",
        "score= accuracy_score(y_test,predictions)\n",
        "print(score)\n",
        "report= classification_report(y_test,predictions)\n",
        "print(report)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[124 368]\n",
            " [118 416]]\n",
            "0.5263157894736842\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.51      0.25      0.34       492\n",
            "           1       0.53      0.78      0.63       534\n",
            "\n",
            "    accuracy                           0.53      1026\n",
            "   macro avg       0.52      0.52      0.48      1026\n",
            "weighted avg       0.52      0.53      0.49      1026\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8iR0t_q56EM"
      },
      "source": [
        "It still looks like our accuracy is quite low, and further optimizations will be required with these approaches.\n",
        "\n",
        "However, first, we're going to try our Naive-Bayes Classifier and see how that performs, and then we're going to examine another architecture that may be better for this particular sentiment analysis problem: an LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjcBqzng8oNg"
      },
      "source": [
        "## Naive-Bayes Classifier\n",
        "\n",
        "First, we'll use our 'bag of words' model to convert text into vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2Qb9z5D5ltE"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "naive = MultinomialNB()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4giyMcl8-c8"
      },
      "source": [
        "countvector = CountVectorizer(ngram_range=(2, 2))\n",
        "train_dataset = countvector.fit_transform(X_train)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx6n-g3A9e38",
        "outputId": "55da1a37-85af-4752-a9b7-90ca2b2684ec"
      },
      "source": [
        "# Fitting Naive Bayes Classifier with training data\n",
        "naive.fit(train_dataset, y_train)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7rpjdxb9tOs"
      },
      "source": [
        "# Predicting on test dataset\n",
        "test_dataset = countvector.transform(X_test)\n",
        "predictions = naive.predict(test_dataset)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8Mg6zoy-IpY",
        "outputId": "15695eeb-0596-4ee9-9f66-0b67f07a3210"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nrx_OY5-MPE",
        "outputId": "12f74dd9-c3db-46b4-bb7e-9db54cd577ee"
      },
      "source": [
        "matrix= confusion_matrix(y_test, predictions)\n",
        "print(matrix)\n",
        "score= accuracy_score(y_test, predictions)\n",
        "print(score)\n",
        "report= classification_report(y_test, predictions)\n",
        "print(report)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[130 362]\n",
            " [121 413]]\n",
            "0.5292397660818714\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.26      0.35       492\n",
            "           1       0.53      0.77      0.63       534\n",
            "\n",
            "    accuracy                           0.53      1026\n",
            "   macro avg       0.53      0.52      0.49      1026\n",
            "weighted avg       0.53      0.53      0.50      1026\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXUII6ZG-eJ8"
      },
      "source": [
        "We could try Naive-Bayes with TF-IDF Vectorization, but I'm not convinced that it's necessarily going to perform any better for this problem. I'd like to take a different approach. First, I want to look at LSTM RNN's, and see how one performs on this dataset.\n",
        "\n",
        "Knowing that LSTM's often need a LOT of data to perform well, I'm then going to see if I can combine the data that I have, preprocess it accordingly, and provide the LSTM with the combined set so it's better able to train. \n",
        "\n",
        "First, though, I'm going to just 'prove concept' with the current dataset that we're working with, and see if our accuracy even marginally improves. It's possible another type of architecture altogether suits us better, so let's see!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZ-n_a6GpWpU"
      },
      "source": [
        "# LSTM with bag of words model & limited data\n",
        "\n",
        "LABELS: \n",
        "\n",
        "0 -> Stock will go down\n",
        "\n",
        "1 -> Stock will go up or hold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sm9iBl_9-2U0"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "import numpy as np\n",
        "import re"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ1QMIO2GY0g"
      },
      "source": [
        "# Set parameters\n",
        "\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X = tokenizer.texts_to_sequences(X_train)\n",
        "X = pad_sequences(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK5Qkn05HHNQ",
        "outputId": "7800cc8c-2360-4e53-a8e0-e9fcae58a80b"
      },
      "source": [
        "# Compose the LSTM network.\n",
        "# embed_dim, lstm_out, batch_size, and dropout_x are hyperparameters that may need to be tweaked in order to optimize output\n",
        "# input_length, which we pass to our Embedding layer, takes the 1st index of our X value -> or, the length of our features?\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embed_dim, input_length = X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 537, 128)          256000    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 537, 128)          0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 197       \n",
            "=================================================================\n",
            "Total params: 510,997\n",
            "Trainable params: 510,997\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNbvoHPgIGg0",
        "outputId": "f9f31081-42d1-4504-8d85-536677f60dc7"
      },
      "source": [
        "batch_size = 32\n",
        "print(type(X))\n",
        "print(type(y_train))\n",
        "model.fit(X, y_train, epochs = 15, batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "Epoch 1/15\n",
            "97/97 - 305s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 2/15\n",
            "97/97 - 304s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 3/15\n",
            "97/97 - 308s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 4/15\n",
            "97/97 - 306s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 5/15\n",
            "97/97 - 306s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 6/15\n",
            "97/97 - 305s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 7/15\n",
            "97/97 - 305s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 8/15\n",
            "97/97 - 305s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 9/15\n",
            "97/97 - 305s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 10/15\n",
            "97/97 - 306s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 11/15\n",
            "97/97 - 306s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 12/15\n",
            "97/97 - 306s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 13/15\n",
            "97/97 - 308s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 14/15\n",
            "97/97 - 307s - loss: 0.0000e+00 - accuracy: 0.5265\n",
            "Epoch 15/15\n",
            "97/97 - 308s - loss: 0.0000e+00 - accuracy: 0.5265\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbc10207110>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e90pAcUpUU4"
      },
      "source": [
        "# LSTM with TF-IDF Vectorization and increased data\n",
        "\n",
        "### Using by-company stock data scraped from Reddit & Yahoo Finance\n",
        "\n",
        "LABELS: \n",
        "\n",
        "0 -> Stock will go down (negative)\n",
        "\n",
        "1 -> Stock will go up (positive)\n",
        "\n",
        "2 -> Stock will hold (neutral)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiig_oSHpmcQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        },
        "outputId": "7f029165-ba5b-4965-bb83-3915492bab31"
      },
      "source": [
        "# Get data & perform sentiment analysis to provide training labels\n",
        "djia_df = pd.read_csv(\"/content/drive/MyDrive/StockStalker/djia_news.csv\")\n",
        "nasdaq_df = pd.read_csv('/content/drive/MyDrive/StockStalker/nasdaq.csv')\n",
        "\n",
        "print(djia_df)\n",
        "print(nasdaq_df)\n",
        "\n",
        "df = pd.concat([djia_df, nasdaq_df], ignore_index=True)\n",
        "df"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      Label Ticker                                           Headline\n",
            "0         0    MMM  Employer who stole nearly $3M in wages from 15...\n",
            "1         1    MMM  Huge new Facebook data leak exposed intimate d...\n",
            "2         0    MMM  A campaign has accelerated to turn a disused r...\n",
            "3         1    MMM  Google launches global human trafficking helpl...\n",
            "4         1    MMM  Over 3m Saudi Women Dont Have ID Cards; Saudi...\n",
            "...     ...    ...                                                ...\n",
            "2376      0    WMT  Walmart dumps e-cigarettes: Largest store in U...\n",
            "2377      0    WMT  Walmart makes a $16 billion bet on India's boo...\n",
            "2378      0    WMT  Walmart raises minimum age to buy tobacco to 2...\n",
            "2379      0    WMT  Walmart Took Over Chile In Only Three Years An...\n",
            "2380      2    WMT  Carla Cheney: Walmart Fired Me For Reporting D...\n",
            "\n",
            "[2381 rows x 3 columns]\n",
            "       Label Ticker                                           Headline\n",
            "0          0      A  @TotesTravel : Airline shares tumble as New Yo...\n",
            "1          1      A  @TotesTravel : American United call off Hong K...\n",
            "2          0      A  @TotesTravel : U.S. airline stocks hit highest...\n",
            "3          1      A  @TotesTravel : American Airlines reaches deal ...\n",
            "4          1      A  @TotesTravel : US airlines Treasury Department...\n",
            "...      ...    ...                                                ...\n",
            "13176      1   ZNGA  Bitcoin Tops $1000 Again as Zynga Accepts Virt...\n",
            "13177      1   ZNGA        Zynga Accepts Bitcoin For Microtransactions\n",
            "13178      1   ZUMZ  Zumiez (ZUMZ) unusual put activity into earnin...\n",
            "13179      1   ZUMZ                           Zumiez Is Going Bankrupt\n",
            "13180      1   ZUMZ                   Zumiez Is Going Bankrupt!! Omfg!\n",
            "\n",
            "[13181 rows x 3 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>MMM</td>\n",
              "      <td>Employer who stole nearly $3M in wages from 15...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>MMM</td>\n",
              "      <td>Huge new Facebook data leak exposed intimate d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>MMM</td>\n",
              "      <td>A campaign has accelerated to turn a disused r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>MMM</td>\n",
              "      <td>Google launches global human trafficking helpl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>MMM</td>\n",
              "      <td>Over 3m Saudi Women Dont Have ID Cards; Saudi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15557</th>\n",
              "      <td>1</td>\n",
              "      <td>ZNGA</td>\n",
              "      <td>Bitcoin Tops $1000 Again as Zynga Accepts Virt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15558</th>\n",
              "      <td>1</td>\n",
              "      <td>ZNGA</td>\n",
              "      <td>Zynga Accepts Bitcoin For Microtransactions</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15559</th>\n",
              "      <td>1</td>\n",
              "      <td>ZUMZ</td>\n",
              "      <td>Zumiez (ZUMZ) unusual put activity into earnin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15560</th>\n",
              "      <td>1</td>\n",
              "      <td>ZUMZ</td>\n",
              "      <td>Zumiez Is Going Bankrupt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15561</th>\n",
              "      <td>1</td>\n",
              "      <td>ZUMZ</td>\n",
              "      <td>Zumiez Is Going Bankrupt!! Omfg!</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15562 rows  3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Label Ticker                                           Headline\n",
              "0          0    MMM  Employer who stole nearly $3M in wages from 15...\n",
              "1          1    MMM  Huge new Facebook data leak exposed intimate d...\n",
              "2          0    MMM  A campaign has accelerated to turn a disused r...\n",
              "3          1    MMM  Google launches global human trafficking helpl...\n",
              "4          1    MMM  Over 3m Saudi Women Dont Have ID Cards; Saudi...\n",
              "...      ...    ...                                                ...\n",
              "15557      1   ZNGA  Bitcoin Tops $1000 Again as Zynga Accepts Virt...\n",
              "15558      1   ZNGA        Zynga Accepts Bitcoin For Microtransactions\n",
              "15559      1   ZUMZ  Zumiez (ZUMZ) unusual put activity into earnin...\n",
              "15560      1   ZUMZ                           Zumiez Is Going Bankrupt\n",
              "15561      1   ZUMZ                   Zumiez Is Going Bankrupt!! Omfg!\n",
              "\n",
              "[15562 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKrISjnjhwvo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "0e4be98e-ec94-426a-97d1-ee0048a72fa6"
      },
      "source": [
        "# Remove punctuation\n",
        "df.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n",
        "\n",
        "df[\"Headline\"] = df[\"Headline\"].str.lower()\n",
        "\n",
        "df[\"Headline\"] = df[\"Ticker\"] + ' ' + df[\"Headline\"]\n",
        "\n",
        "df.head(10)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Ticker</th>\n",
              "      <th>Headline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>MMM</td>\n",
              "      <td>MMM employer who stole nearly   m in wages fro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>MMM</td>\n",
              "      <td>MMM huge new facebook data leak exposed intima...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>MMM</td>\n",
              "      <td>MMM a campaign has accelerated to turn a disus...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>MMM</td>\n",
              "      <td>MMM google launches global human trafficking h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>MMM</td>\n",
              "      <td>MMM over  m saudi women don t have id cards  s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>MMM</td>\n",
              "      <td>MMM boris johnson promises tax cut for  m high...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>MMM</td>\n",
              "      <td>MMM canada spends   m to stop female genital m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>MMM</td>\n",
              "      <td>MMM u s  accuses china of detaining up to  m x...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>MMM</td>\n",
              "      <td>MMM more than   m raised for humboldt broncos ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>MMM</td>\n",
              "      <td>MMM australian property giant meriton has been...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Label Ticker                                           Headline\n",
              "0      0    MMM  MMM employer who stole nearly   m in wages fro...\n",
              "1      1    MMM  MMM huge new facebook data leak exposed intima...\n",
              "2      0    MMM  MMM a campaign has accelerated to turn a disus...\n",
              "3      1    MMM  MMM google launches global human trafficking h...\n",
              "4      1    MMM  MMM over  m saudi women don t have id cards  s...\n",
              "5      1    MMM  MMM boris johnson promises tax cut for  m high...\n",
              "6      0    MMM  MMM canada spends   m to stop female genital m...\n",
              "7      1    MMM  MMM u s  accuses china of detaining up to  m x...\n",
              "8      0    MMM  MMM more than   m raised for humboldt broncos ...\n",
              "9      0    MMM  MMM australian property giant meriton has been..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAE_II_ntwEA",
        "outputId": "7f4e242c-d75a-4624-bb33-31ac80d147fe"
      },
      "source": [
        "# One-hot encode the labels\n",
        "from keras.utils.np_utils import to_categorical\n",
        "labels = to_categorical(df[\"Label\"], num_classes=3)\n",
        "\n",
        "print(labels[:10])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEmy74idl0_4",
        "outputId": "05ed84c3-1af1-4f4a-88a0-ad485f9e4b76"
      },
      "source": [
        "# Split our train and test data\n",
        "\n",
        "X = df.drop('Label', axis=1)\n",
        "y = df['Label']\n",
        " \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11671, 2)\n",
            "(11671, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkNYT-mDnSk8",
        "outputId": "08380ae3-d585-42f3-e941-c3956c0dd28e"
      },
      "source": [
        "# Set parameters\n",
        "\n",
        "max_features = 8000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "print(f\"tokenizer: {tokenizer}\")\n",
        "tokenizer.fit_on_texts(X_train['Headline'])\n",
        "X = tokenizer.texts_to_sequences(X_train['Headline'])\n",
        "print(f\"X size after to_sequences: {len(X)}\")\n",
        "\n",
        "X = pad_sequences(X)\n",
        "\n",
        "print(X.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenizer: <keras_preprocessing.text.Tokenizer object at 0x7fb7bc5f5d50>\n",
            "X size after to_sequences: 11671\n",
            "(11671, 66)\n",
            "(11671, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjNPvvVEngXw",
        "outputId": "3eeaa968-6762-420a-f420-19deecb7f15c"
      },
      "source": [
        "# Compose the LSTM network.\n",
        "# embed_dim, lstm_out, batch_size, and dropout_x are hyperparameters that may need to be tweaked in order to optimize output\n",
        "# input_length, which we pass to our Embedding layer, takes the 1st index of our X value -> or, the length of our features?\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 66, 128)           1024000   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 66, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 196)               254800    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 591       \n",
            "=================================================================\n",
            "Total params: 1,279,391\n",
            "Trainable params: 1,279,391\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZDuU91Rnn14",
        "outputId": "5f913f70-c473-4a45-fe9d-3bc21cb225b0"
      },
      "source": [
        "# #find the maximum vocabulary size\n",
        "# voc_size = (flows_scaled.max()+1).astype('int64')\n",
        "# print(voc_size)\n",
        "\n",
        "batch_size = 32\n",
        "print(type(X))\n",
        "print(type(y_train))\n",
        "model.fit(X, y_train, epochs = 100, batch_size=batch_size, verbose = 2, validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "Epoch 1/100\n",
            "256/256 - 91s - loss: 0.7394 - accuracy: 0.6445 - val_loss: 0.7221 - val_accuracy: 0.6331\n",
            "Epoch 2/100\n",
            "256/256 - 85s - loss: 0.6238 - accuracy: 0.7058 - val_loss: 0.6910 - val_accuracy: 0.6513\n",
            "Epoch 3/100\n",
            "256/256 - 85s - loss: 0.4465 - accuracy: 0.8070 - val_loss: 0.7292 - val_accuracy: 0.6528\n",
            "Epoch 4/100\n",
            "256/256 - 87s - loss: 0.3182 - accuracy: 0.8704 - val_loss: 0.9194 - val_accuracy: 0.6682\n",
            "Epoch 5/100\n",
            "256/256 - 85s - loss: 0.2376 - accuracy: 0.9083 - val_loss: 0.9225 - val_accuracy: 0.6699\n",
            "Epoch 6/100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzlZY9k1l36Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1UvY5dWA4Hk"
      },
      "source": [
        "Citations:\n",
        "\n",
        "Sun, J. (2016, August). Daily News for Stock Market Prediction, Version 1. Retrieved (2021, March) from https://www.kaggle.com/aaron7sun/stocknews."
      ]
    }
  ]
}
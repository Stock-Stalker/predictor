{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HN95rcdD0hY"
   },
   "source": [
    "# Estimating whether a stock will go up, down, or hold based on news headlines\n",
    "\n",
    "To begin our exploration, we will use a Random Forest Classifier, a Naive-Bayes classifier with a \"bag of words\" approach, and an LSTM Recurrent Neural Network to see which gives us the best performance on a \"test\" dataset from Kaggle which closely approximates the data we will use on our actual business case.\n",
    "\n",
    "LABELS: \n",
    "\n",
    "For our initial approaches, prior to trying our neural network's hand at this problem, we'll be using a binary classification, where: \n",
    "\n",
    "0 -> Stocks will go down\n",
    "\n",
    "1 -> Stocks will go up or hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "iyr3i6ea8XeI"
   },
   "outputs": [],
   "source": [
    "# Import numpy and pandas to start\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BmydtDUxA35g",
    "outputId": "4bdbde43-e6af-4932-e3b3-04628898a032"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NJR9Hod7DBqs"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/StockStalker/Data/Stock_Dataa.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7f1037948f44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read in our first dataset as df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/StockStalker/Data/Stock_Dataa.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ISO-8859-1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             )\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \"\"\"\n\u001b[0;32m-> 1362\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"replace\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/StockStalker/Data/Stock_Dataa.csv'"
     ]
    }
   ],
   "source": [
    "# Read in our first dataset as df\n",
    "df = pd.read_csv('/content/drive/MyDrive/StockStalker/Data/Stock_Dataa.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "id": "mHoB2Yz4DYLU",
    "outputId": "0ffa5fed-6260-4433-9b5a-224bd68a0d30"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>Top9</th>\n",
       "      <th>Top10</th>\n",
       "      <th>Top11</th>\n",
       "      <th>Top12</th>\n",
       "      <th>Top13</th>\n",
       "      <th>Top14</th>\n",
       "      <th>Top15</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>A 'hindrance to operations': extracts from the...</td>\n",
       "      <td>Scorecard</td>\n",
       "      <td>Hughes' instant hit buoys Blues</td>\n",
       "      <td>Jack gets his skates on at ice-cold Alex</td>\n",
       "      <td>Chaos as Maracana builds up for United</td>\n",
       "      <td>Depleted Leicester prevail as Elliott spoils E...</td>\n",
       "      <td>Hungry Spurs sense rich pickings</td>\n",
       "      <td>Gunners so wide of an easy target</td>\n",
       "      <td>Derby raise a glass to Strupar's debut double</td>\n",
       "      <td>Southgate strikes, Leeds pay the penalty</td>\n",
       "      <td>Hammers hand Robson a youthful lesson</td>\n",
       "      <td>Saints party like it's 1999</td>\n",
       "      <td>Wear wolves have turned into lambs</td>\n",
       "      <td>Stump mike catches testy Gough's taunt</td>\n",
       "      <td>Langer escapes to hit 167</td>\n",
       "      <td>Flintoff injury piles on woe for England</td>\n",
       "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
       "      <td>Kohl's successor drawn into scandal</td>\n",
       "      <td>The difference between men and women</td>\n",
       "      <td>Sara Denver, nurse turned solicitor</td>\n",
       "      <td>Diana's landmine crusade put Tories in a panic</td>\n",
       "      <td>Yeltsin's resignation caught opposition flat-f...</td>\n",
       "      <td>Russian roulette</td>\n",
       "      <td>Sold out</td>\n",
       "      <td>Recovering a title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>Scorecard</td>\n",
       "      <td>The best lake scene</td>\n",
       "      <td>Leader: German sleaze inquiry</td>\n",
       "      <td>Cheerio, boyo</td>\n",
       "      <td>The main recommendations</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>Hopkins 'furious' at Foster's lack of Hannibal...</td>\n",
       "      <td>Has Cubie killed fees?</td>\n",
       "      <td>A tale of two tails</td>\n",
       "      <td>I say what I like and I like what I say</td>\n",
       "      <td>Elbows, Eyes and Nipples</td>\n",
       "      <td>Task force to assess risk of asteroid collision</td>\n",
       "      <td>How I found myself at last</td>\n",
       "      <td>On the critical list</td>\n",
       "      <td>The timing of their lives</td>\n",
       "      <td>Dear doctor</td>\n",
       "      <td>Irish court halts IRA man's extradition to Nor...</td>\n",
       "      <td>Burundi peace initiative fades after rebels re...</td>\n",
       "      <td>PE points the way forward to the ECB</td>\n",
       "      <td>Campaigners keep up pressure on Nazi war crime...</td>\n",
       "      <td>Jane Ratcliffe</td>\n",
       "      <td>Yet more things you wouldn't know without the ...</td>\n",
       "      <td>Millennium bug fails to bite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>0</td>\n",
       "      <td>Coventry caught on counter by Flo</td>\n",
       "      <td>United's rivals on the road to Rio</td>\n",
       "      <td>Thatcher issues defence before trial by video</td>\n",
       "      <td>Police help Smith lay down the law at Everton</td>\n",
       "      <td>Tale of Trautmann bears two more retellings</td>\n",
       "      <td>England on the rack</td>\n",
       "      <td>Pakistan retaliate with call for video of Walsh</td>\n",
       "      <td>Cullinan continues his Cape monopoly</td>\n",
       "      <td>McGrath puts India out of their misery</td>\n",
       "      <td>Blair Witch bandwagon rolls on</td>\n",
       "      <td>Pele turns up heat on Ferguson</td>\n",
       "      <td>Party divided over Kohl slush fund scandal</td>\n",
       "      <td>Manchester United (England)</td>\n",
       "      <td>Women in record South Pole walk</td>\n",
       "      <td>Vasco da Gama (Brazil)</td>\n",
       "      <td>South Melbourne (Australia)</td>\n",
       "      <td>Necaxa (Mexico)</td>\n",
       "      <td>Real Madrid (Spain)</td>\n",
       "      <td>Raja Casablanca (Morocco)</td>\n",
       "      <td>Corinthians (Brazil)</td>\n",
       "      <td>Tony's pet project</td>\n",
       "      <td>Al Nassr (Saudi Arabia)</td>\n",
       "      <td>Ideal Holmes show</td>\n",
       "      <td>Pinochet leaves hospital after tests</td>\n",
       "      <td>Useful links</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>1</td>\n",
       "      <td>Pilgrim knows how to progress</td>\n",
       "      <td>Thatcher facing ban</td>\n",
       "      <td>McIlroy calls for Irish fighting spirit</td>\n",
       "      <td>Leicester bin stadium blueprint</td>\n",
       "      <td>United braced for Mexican wave</td>\n",
       "      <td>Auntie back in fashion, even if the dress look...</td>\n",
       "      <td>Shoaib appeal goes to the top</td>\n",
       "      <td>Hussain hurt by 'shambles' but lays blame on e...</td>\n",
       "      <td>England's decade of disasters</td>\n",
       "      <td>Revenge is sweet for jubilant Cronje</td>\n",
       "      <td>Our choice, not theirs</td>\n",
       "      <td>Profile of former US Nazi Party officer Willia...</td>\n",
       "      <td>New evidence shows record of war crimes suspec...</td>\n",
       "      <td>The rise of the supernerds</td>\n",
       "      <td>Written on the body</td>\n",
       "      <td>Putin admits Yeltsin quit to give him a head s...</td>\n",
       "      <td>BBC worst hit as digital TV begins to bite</td>\n",
       "      <td>How much can you pay for...</td>\n",
       "      <td>Christmas glitches</td>\n",
       "      <td>Upending a table, Chopping a line and Scoring ...</td>\n",
       "      <td>Scientific evidence 'unreliable', defence claims</td>\n",
       "      <td>Fusco wins judicial review in extradition case</td>\n",
       "      <td>Rebels thwart Russian advance</td>\n",
       "      <td>Blair orders shake-up of failing NHS</td>\n",
       "      <td>Lessons of law's hard heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-01-07</td>\n",
       "      <td>1</td>\n",
       "      <td>Hitches and Horlocks</td>\n",
       "      <td>Beckham off but United survive</td>\n",
       "      <td>Breast cancer screening</td>\n",
       "      <td>Alan Parker</td>\n",
       "      <td>Guardian readers: are you all whingers?</td>\n",
       "      <td>Hollywood Beyond</td>\n",
       "      <td>Ashes and diamonds</td>\n",
       "      <td>Whingers - a formidable minority</td>\n",
       "      <td>Alan Parker - part two</td>\n",
       "      <td>Thuggery, Toxins and Ties</td>\n",
       "      <td>Met faces fresh attack on race crime</td>\n",
       "      <td>Everton fans top racist 'league of shame'</td>\n",
       "      <td>Our breasts, ourselves</td>\n",
       "      <td>Russia's new boss has an extremely strange his...</td>\n",
       "      <td>Always and forever</td>\n",
       "      <td>Most everywhere:  UDIs</td>\n",
       "      <td>Most wanted:  Chloe lunettes</td>\n",
       "      <td>Return of the cane 'completely off the agenda'</td>\n",
       "      <td>From Sleepy Hollow to Greeneland</td>\n",
       "      <td>Blunkett outlines vision for over 11s</td>\n",
       "      <td>Embattled Dobson attacks 'play now, pay later'...</td>\n",
       "      <td>Doom and the Dome</td>\n",
       "      <td>What is the north-south divide?</td>\n",
       "      <td>Aitken released from jail</td>\n",
       "      <td>Gone aloft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  ...                         Top25\n",
       "0  2000-01-03  ...            Recovering a title\n",
       "1  2000-01-04  ...  Millennium bug fails to bite\n",
       "2  2000-01-05  ...                  Useful links\n",
       "3  2000-01-06  ...   Lessons of law's hard heart\n",
       "4  2000-01-07  ...                    Gone aloft\n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "id": "FCRU6IjNDaw8",
    "outputId": "52099532-6c2a-417e-a307-7f8867673b8d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Label</th>\n",
       "      <th>Top1</th>\n",
       "      <th>Top2</th>\n",
       "      <th>Top3</th>\n",
       "      <th>Top4</th>\n",
       "      <th>Top5</th>\n",
       "      <th>Top6</th>\n",
       "      <th>Top7</th>\n",
       "      <th>Top8</th>\n",
       "      <th>Top9</th>\n",
       "      <th>Top10</th>\n",
       "      <th>Top11</th>\n",
       "      <th>Top12</th>\n",
       "      <th>Top13</th>\n",
       "      <th>Top14</th>\n",
       "      <th>Top15</th>\n",
       "      <th>Top16</th>\n",
       "      <th>Top17</th>\n",
       "      <th>Top18</th>\n",
       "      <th>Top19</th>\n",
       "      <th>Top20</th>\n",
       "      <th>Top21</th>\n",
       "      <th>Top22</th>\n",
       "      <th>Top23</th>\n",
       "      <th>Top24</th>\n",
       "      <th>Top25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>2016-06-27</td>\n",
       "      <td>0</td>\n",
       "      <td>Barclays and RBS shares suspended from trading...</td>\n",
       "      <td>Pope says Church should ask forgiveness from g...</td>\n",
       "      <td>Poland 'shocked' by xenophobic abuse of Poles ...</td>\n",
       "      <td>There will be no second referendum, cabinet ag...</td>\n",
       "      <td>Scotland welcome to join EU, Merkel ally says</td>\n",
       "      <td>Sterling dips below Friday's 31-year low amid ...</td>\n",
       "      <td>No negative news about South African President...</td>\n",
       "      <td>Surge in Hate Crimes in the U.K. Following U.K...</td>\n",
       "      <td>Weapons shipped into Jordan by the CIA and Sau...</td>\n",
       "      <td>Angela Merkel said the U.K. must file exit pap...</td>\n",
       "      <td>In a birth offering hope to a threatened speci...</td>\n",
       "      <td>Sky News Journalist Left Speechless As Leave M...</td>\n",
       "      <td>Giant panda in Macau gives birth to twins</td>\n",
       "      <td>Get out now: EU leader tells Britain it must i...</td>\n",
       "      <td>Sea turtle 'beaten and left for dead' on beach...</td>\n",
       "      <td>German lawyers to probe Erdogan over alleged w...</td>\n",
       "      <td>Boris Johnson says the UK will continue to \"in...</td>\n",
       "      <td>Richard Branson is calling on the UK governmen...</td>\n",
       "      <td>Turkey 'sorry for downing Russian jet'</td>\n",
       "      <td>Edward Snowden lawyer vows new push for pardon...</td>\n",
       "      <td>Brexit opinion poll reveals majority don't wan...</td>\n",
       "      <td>Conservative MP Leave Campaigner: \"The leave c...</td>\n",
       "      <td>Economists predict UK recession, further weake...</td>\n",
       "      <td>New EU 'superstate plan by France, Germany: Cr...</td>\n",
       "      <td>Pakistani clerics declare transgender marriage...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1</td>\n",
       "      <td>2,500 Scientists To Australia: If You Want To ...</td>\n",
       "      <td>The personal details of 112,000 French police ...</td>\n",
       "      <td>S&amp;amp;P cuts United Kingdom sovereign credit r...</td>\n",
       "      <td>Huge helium deposit found in Africa</td>\n",
       "      <td>CEO of the South African state broadcaster qui...</td>\n",
       "      <td>Brexit cost investors $2 trillion, the worst o...</td>\n",
       "      <td>Hong Kong democracy activists call for return ...</td>\n",
       "      <td>Brexit: Iceland president says UK can join 'tr...</td>\n",
       "      <td>UK's Osborne: 'Absolutely' going to have to cu...</td>\n",
       "      <td>'Do not let Scotland down now' : Scottish MEP ...</td>\n",
       "      <td>British pound could hit history-making dollar ...</td>\n",
       "      <td>Merkel vows to strengthen EU, tells UK no 'che...</td>\n",
       "      <td>\"Ryanair will not deploy new aircraft on route...</td>\n",
       "      <td>People, ever more greedy and stupid, destroy t...</td>\n",
       "      <td>Siemens freezes new UK wind power investment f...</td>\n",
       "      <td>US, Canada and Mexico pledge 50% of power from...</td>\n",
       "      <td>There is increasing evidence that Australia is...</td>\n",
       "      <td>Richard Branson, the founder of Virgin Group, ...</td>\n",
       "      <td>37,000-yr-old skull from Borneo reveals surpri...</td>\n",
       "      <td>Palestinians stone Western Wall worshipers; po...</td>\n",
       "      <td>Jean-Claude Juncker asks Farage: Why are you h...</td>\n",
       "      <td>\"Romanians for Remainians\" offering a new home...</td>\n",
       "      <td>Brexit: Gibraltar in talks with Scotland to st...</td>\n",
       "      <td>8 Suicide Bombers Strike Lebanon</td>\n",
       "      <td>Mexico's security forces routinely use 'sexual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>2016-06-29</td>\n",
       "      <td>1</td>\n",
       "      <td>Explosion At Airport In Istanbul</td>\n",
       "      <td>Yemeni former president: Terrorism is the offs...</td>\n",
       "      <td>UK must accept freedom of movement to access E...</td>\n",
       "      <td>Devastated: scientists too late to captive bre...</td>\n",
       "      <td>British Labor Party leader Jeremy Corbyn loses...</td>\n",
       "      <td>A Muslim Shop in the UK Was Just Firebombed Wh...</td>\n",
       "      <td>Mexican Authorities Sexually Torture Women in ...</td>\n",
       "      <td>UK shares and pound continue to recover</td>\n",
       "      <td>Iceland historian Johannesson wins presidentia...</td>\n",
       "      <td>99-Million-Yr-Old Bird Wings Found Encased in ...</td>\n",
       "      <td>A chatbot programmed by a British teenager has...</td>\n",
       "      <td>The Philippine president-elect said Monday he ...</td>\n",
       "      <td>Former Belgian Prime Minister ridicules Nigel ...</td>\n",
       "      <td>Brexiteer Nigel Farage To EU: 'You're Not Laug...</td>\n",
       "      <td>Islamic State bombings in southern Yemen kill ...</td>\n",
       "      <td>Escape Tunnel, Dug by Hand, Is Found at Holoca...</td>\n",
       "      <td>The land under Beijing is sinking by as much a...</td>\n",
       "      <td>Car bomb and Anti-Islamic attack on Mosque in ...</td>\n",
       "      <td>Emaciated lions in Taiz Zoo are trapped in blo...</td>\n",
       "      <td>Rupert Murdoch describes Brexit as 'wonderful'...</td>\n",
       "      <td>More than 40 killed in Yemen suicide attacks</td>\n",
       "      <td>Google Found Disastrous Symantec and Norton Vu...</td>\n",
       "      <td>Extremist violence on the rise in Germany: Dom...</td>\n",
       "      <td>BBC News: Labour MPs pass Corbyn no-confidence...</td>\n",
       "      <td>Tiny New Zealand town with 'too many jobs' lau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>1</td>\n",
       "      <td>Jamaica proposes marijuana dispensers for tour...</td>\n",
       "      <td>Stephen Hawking says pollution and 'stupidity'...</td>\n",
       "      <td>Boris Johnson says he will not run for Tory pa...</td>\n",
       "      <td>Six gay men in Ivory Coast were abused and for...</td>\n",
       "      <td>Switzerland denies citizenship to Muslim immig...</td>\n",
       "      <td>Palestinian terrorist stabs israeli teen girl ...</td>\n",
       "      <td>Puerto Rico will default on $1 billion of debt...</td>\n",
       "      <td>Republic of Ireland fans to be awarded medal f...</td>\n",
       "      <td>Afghan suicide bomber 'kills up to 40' - BBC News</td>\n",
       "      <td>US airstrikes kill at least 250 ISIS fighters ...</td>\n",
       "      <td>Turkish Cop Who Took Down Istanbul Gunman Hail...</td>\n",
       "      <td>Cannabis compounds could treat Alzheimer's by ...</td>\n",
       "      <td>Japan's top court has approved blanket surveil...</td>\n",
       "      <td>CIA Gave Romania Millions to Host Secret Prisons</td>\n",
       "      <td>Groups urge U.N. to suspend Saudi Arabia from ...</td>\n",
       "      <td>Googles free wifi at Indian railway stations i...</td>\n",
       "      <td>Mounting evidence suggests 'hobbits' were wipe...</td>\n",
       "      <td>The men who carried out Tuesday's terror attac...</td>\n",
       "      <td>Calls to suspend Saudi Arabia from UN Human Ri...</td>\n",
       "      <td>More Than 100 Nobel Laureates Call Out Greenpe...</td>\n",
       "      <td>British pedophile sentenced to 85 years in US ...</td>\n",
       "      <td>US permitted 1,200 offshore fracks in Gulf of ...</td>\n",
       "      <td>We will be swimming in ridicule - French beach...</td>\n",
       "      <td>UEFA says no minutes of silence for Istanbul v...</td>\n",
       "      <td>Law Enforcement Sources: Gun Used in Paris Ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>A 117-year-old woman in Mexico City finally re...</td>\n",
       "      <td>IMF chief backs Athens as permanent Olympic host</td>\n",
       "      <td>The president of France says if Brexit won, so...</td>\n",
       "      <td>British Man Who Must Give Police 24 Hours' Not...</td>\n",
       "      <td>100+ Nobel laureates urge Greenpeace to stop o...</td>\n",
       "      <td>Brazil: Huge spike in number of police killing...</td>\n",
       "      <td>Austria's highest court annuls presidential el...</td>\n",
       "      <td>Facebook wins privacy case, can track any Belg...</td>\n",
       "      <td>Switzerland denies Muslim girls citizenship af...</td>\n",
       "      <td>China kills millions of innocent meditators fo...</td>\n",
       "      <td>France Cracks Down on Factory Farms - A viral ...</td>\n",
       "      <td>Abbas PLO Faction Calls Killer of 13-Year-Old ...</td>\n",
       "      <td>Taiwanese warship accidentally fires missile t...</td>\n",
       "      <td>Iran celebrates American Human Rights Week, mo...</td>\n",
       "      <td>U.N. panel moves to curb bias against L.G.B.T....</td>\n",
       "      <td>The United States has placed Myanmar, Uzbekist...</td>\n",
       "      <td>S&amp;amp;P revises European Union credit rating t...</td>\n",
       "      <td>India gets $1 billion loan from World Bank for...</td>\n",
       "      <td>U.S. sailors detained by Iran spoke too much u...</td>\n",
       "      <td>Mass fish kill in Vietnam solved as Taiwan ste...</td>\n",
       "      <td>Philippines president Rodrigo Duterte urges pe...</td>\n",
       "      <td>Spain arrests three Pakistanis accused of prom...</td>\n",
       "      <td>Venezuela, where anger over food shortages is ...</td>\n",
       "      <td>A Hindu temple worker has been killed by three...</td>\n",
       "      <td>Ozone layer hole seems to be healing - US &amp;amp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  ...                                              Top25\n",
       "4096  2016-06-27  ...  Pakistani clerics declare transgender marriage...\n",
       "4097  2016-06-28  ...  Mexico's security forces routinely use 'sexual...\n",
       "4098  2016-06-29  ...  Tiny New Zealand town with 'too many jobs' lau...\n",
       "4099  2016-06-30  ...  Law Enforcement Sources: Gun Used in Paris Ter...\n",
       "4100  2016-07-01  ...  Ozone layer hole seems to be healing - US &amp...\n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at last 5 rows\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wh9bfHATEo6g"
   },
   "source": [
    "Data preprocessing for our text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "id": "aegGn6IAEr5I",
    "outputId": "d7c6343a-72fa-4e91-cb38-a81aa333b3a9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A  hindrance to operations   extracts from the...</td>\n",
       "      <td>Scorecard</td>\n",
       "      <td>Hughes  instant hit buoys Blues</td>\n",
       "      <td>Jack gets his skates on at ice cold Alex</td>\n",
       "      <td>Chaos as Maracana builds up for United</td>\n",
       "      <td>Depleted Leicester prevail as Elliott spoils E...</td>\n",
       "      <td>Hungry Spurs sense rich pickings</td>\n",
       "      <td>Gunners so wide of an easy target</td>\n",
       "      <td>Derby raise a glass to Strupar s debut double</td>\n",
       "      <td>Southgate strikes  Leeds pay the penalty</td>\n",
       "      <td>Hammers hand Robson a youthful lesson</td>\n",
       "      <td>Saints party like it s</td>\n",
       "      <td>Wear wolves have turned into lambs</td>\n",
       "      <td>Stump mike catches testy Gough s taunt</td>\n",
       "      <td>Langer escapes to hit</td>\n",
       "      <td>Flintoff injury piles on woe for England</td>\n",
       "      <td>Hunters threaten Jospin with new battle of the...</td>\n",
       "      <td>Kohl s successor drawn into scandal</td>\n",
       "      <td>The difference between men and women</td>\n",
       "      <td>Sara Denver  nurse turned solicitor</td>\n",
       "      <td>Diana s landmine crusade put Tories in a panic</td>\n",
       "      <td>Yeltsin s resignation caught opposition flat f...</td>\n",
       "      <td>Russian roulette</td>\n",
       "      <td>Sold out</td>\n",
       "      <td>Recovering a title</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scorecard</td>\n",
       "      <td>The best lake scene</td>\n",
       "      <td>Leader  German sleaze inquiry</td>\n",
       "      <td>Cheerio  boyo</td>\n",
       "      <td>The main recommendations</td>\n",
       "      <td>Has Cubie killed fees</td>\n",
       "      <td>Has Cubie killed fees</td>\n",
       "      <td>Has Cubie killed fees</td>\n",
       "      <td>Hopkins  furious  at Foster s lack of Hannibal...</td>\n",
       "      <td>Has Cubie killed fees</td>\n",
       "      <td>A tale of two tails</td>\n",
       "      <td>I say what I like and I like what I say</td>\n",
       "      <td>Elbows  Eyes and Nipples</td>\n",
       "      <td>Task force to assess risk of asteroid collision</td>\n",
       "      <td>How I found myself at last</td>\n",
       "      <td>On the critical list</td>\n",
       "      <td>The timing of their lives</td>\n",
       "      <td>Dear doctor</td>\n",
       "      <td>Irish court halts IRA man s extradition to Nor...</td>\n",
       "      <td>Burundi peace initiative fades after rebels re...</td>\n",
       "      <td>PE points the way forward to the ECB</td>\n",
       "      <td>Campaigners keep up pressure on Nazi war crime...</td>\n",
       "      <td>Jane Ratcliffe</td>\n",
       "      <td>Yet more things you wouldn t know without the ...</td>\n",
       "      <td>Millennium bug fails to bite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coventry caught on counter by Flo</td>\n",
       "      <td>United s rivals on the road to Rio</td>\n",
       "      <td>Thatcher issues defence before trial by video</td>\n",
       "      <td>Police help Smith lay down the law at Everton</td>\n",
       "      <td>Tale of Trautmann bears two more retellings</td>\n",
       "      <td>England on the rack</td>\n",
       "      <td>Pakistan retaliate with call for video of Walsh</td>\n",
       "      <td>Cullinan continues his Cape monopoly</td>\n",
       "      <td>McGrath puts India out of their misery</td>\n",
       "      <td>Blair Witch bandwagon rolls on</td>\n",
       "      <td>Pele turns up heat on Ferguson</td>\n",
       "      <td>Party divided over Kohl slush fund scandal</td>\n",
       "      <td>Manchester United  England</td>\n",
       "      <td>Women in record South Pole walk</td>\n",
       "      <td>Vasco da Gama  Brazil</td>\n",
       "      <td>South Melbourne  Australia</td>\n",
       "      <td>Necaxa  Mexico</td>\n",
       "      <td>Real Madrid  Spain</td>\n",
       "      <td>Raja Casablanca  Morocco</td>\n",
       "      <td>Corinthians  Brazil</td>\n",
       "      <td>Tony s pet project</td>\n",
       "      <td>Al Nassr  Saudi Arabia</td>\n",
       "      <td>Ideal Holmes show</td>\n",
       "      <td>Pinochet leaves hospital after tests</td>\n",
       "      <td>Useful links</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pilgrim knows how to progress</td>\n",
       "      <td>Thatcher facing ban</td>\n",
       "      <td>McIlroy calls for Irish fighting spirit</td>\n",
       "      <td>Leicester bin stadium blueprint</td>\n",
       "      <td>United braced for Mexican wave</td>\n",
       "      <td>Auntie back in fashion  even if the dress look...</td>\n",
       "      <td>Shoaib appeal goes to the top</td>\n",
       "      <td>Hussain hurt by  shambles  but lays blame on e...</td>\n",
       "      <td>England s decade of disasters</td>\n",
       "      <td>Revenge is sweet for jubilant Cronje</td>\n",
       "      <td>Our choice  not theirs</td>\n",
       "      <td>Profile of former US Nazi Party officer Willia...</td>\n",
       "      <td>New evidence shows record of war crimes suspec...</td>\n",
       "      <td>The rise of the supernerds</td>\n",
       "      <td>Written on the body</td>\n",
       "      <td>Putin admits Yeltsin quit to give him a head s...</td>\n",
       "      <td>BBC worst hit as digital TV begins to bite</td>\n",
       "      <td>How much can you pay for</td>\n",
       "      <td>Christmas glitches</td>\n",
       "      <td>Upending a table  Chopping a line and Scoring ...</td>\n",
       "      <td>Scientific evidence  unreliable   defence claims</td>\n",
       "      <td>Fusco wins judicial review in extradition case</td>\n",
       "      <td>Rebels thwart Russian advance</td>\n",
       "      <td>Blair orders shake up of failing NHS</td>\n",
       "      <td>Lessons of law s hard heart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hitches and Horlocks</td>\n",
       "      <td>Beckham off but United survive</td>\n",
       "      <td>Breast cancer screening</td>\n",
       "      <td>Alan Parker</td>\n",
       "      <td>Guardian readers  are you all whingers</td>\n",
       "      <td>Hollywood Beyond</td>\n",
       "      <td>Ashes and diamonds</td>\n",
       "      <td>Whingers   a formidable minority</td>\n",
       "      <td>Alan Parker   part two</td>\n",
       "      <td>Thuggery  Toxins and Ties</td>\n",
       "      <td>Met faces fresh attack on race crime</td>\n",
       "      <td>Everton fans top racist  league of shame</td>\n",
       "      <td>Our breasts  ourselves</td>\n",
       "      <td>Russia s new boss has an extremely strange his...</td>\n",
       "      <td>Always and forever</td>\n",
       "      <td>Most everywhere   UDIs</td>\n",
       "      <td>Most wanted   Chloe lunettes</td>\n",
       "      <td>Return of the cane  completely off the agenda</td>\n",
       "      <td>From Sleepy Hollow to Greeneland</td>\n",
       "      <td>Blunkett outlines vision for over   s</td>\n",
       "      <td>Embattled Dobson attacks  play now  pay later ...</td>\n",
       "      <td>Doom and the Dome</td>\n",
       "      <td>What is the north south divide</td>\n",
       "      <td>Aitken released from jail</td>\n",
       "      <td>Gone aloft</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  ...                            24\n",
       "0  A  hindrance to operations   extracts from the...  ...            Recovering a title\n",
       "1                                          Scorecard  ...  Millennium bug fails to bite\n",
       "2                  Coventry caught on counter by Flo  ...                  Useful links\n",
       "3                      Pilgrim knows how to progress  ...   Lessons of law s hard heart\n",
       "4                               Hitches and Horlocks  ...                    Gone aloft\n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "data = df.iloc[:,2:27]\n",
    "data.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n",
    "\n",
    "# Also, I'm going to rename our columns for ease of access\n",
    "list1 = [i for i in range(25)]\n",
    "new_index = [str(i) for i in list1]\n",
    "data.columns = new_index\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "Zh7NJJudFAQQ",
    "outputId": "46cff782-ceaf-4463-8619-af43d38ed353"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a  hindrance to operations   extracts from the...</td>\n",
       "      <td>scorecard</td>\n",
       "      <td>hughes  instant hit buoys blues</td>\n",
       "      <td>jack gets his skates on at ice cold alex</td>\n",
       "      <td>chaos as maracana builds up for united</td>\n",
       "      <td>depleted leicester prevail as elliott spoils e...</td>\n",
       "      <td>hungry spurs sense rich pickings</td>\n",
       "      <td>gunners so wide of an easy target</td>\n",
       "      <td>derby raise a glass to strupar s debut double</td>\n",
       "      <td>southgate strikes  leeds pay the penalty</td>\n",
       "      <td>hammers hand robson a youthful lesson</td>\n",
       "      <td>saints party like it s</td>\n",
       "      <td>wear wolves have turned into lambs</td>\n",
       "      <td>stump mike catches testy gough s taunt</td>\n",
       "      <td>langer escapes to hit</td>\n",
       "      <td>flintoff injury piles on woe for england</td>\n",
       "      <td>hunters threaten jospin with new battle of the...</td>\n",
       "      <td>kohl s successor drawn into scandal</td>\n",
       "      <td>the difference between men and women</td>\n",
       "      <td>sara denver  nurse turned solicitor</td>\n",
       "      <td>diana s landmine crusade put tories in a panic</td>\n",
       "      <td>yeltsin s resignation caught opposition flat f...</td>\n",
       "      <td>russian roulette</td>\n",
       "      <td>sold out</td>\n",
       "      <td>recovering a title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  ...                  24\n",
       "0  a  hindrance to operations   extracts from the...  ...  recovering a title\n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the headlines into lowercase\n",
    "# so all words are treated the same (apple and Apple, for example)\n",
    "for index in new_index:\n",
    "  data[index] = data[index].str.lower()\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "0PCfedbvGdP0",
    "outputId": "c3da2112-2078-45d5-d7d8-50de163a64ca"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'scorecard the best lake scene leader  german sleaze inquiry cheerio  boyo the main recommendations has cubie killed fees  has cubie killed fees  has cubie killed fees  hopkins  furious  at foster s lack of hannibal appetite has cubie killed fees  a tale of two tails i say what i like and i like what i say elbows  eyes and nipples task force to assess risk of asteroid collision how i found myself at last on the critical list the timing of their lives dear doctor irish court halts ira man s extradition to northern ireland burundi peace initiative fades after rebels reject mandela as mediator pe points the way forward to the ecb campaigners keep up pressure on nazi war crimes suspect jane ratcliffe yet more things you wouldn t know without the movies millennium bug fails to bite'"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining the first 25 headlines of first record\n",
    "' '.join(str(x) for x in data.iloc[1,0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "8Ro3F-DaGn6o",
    "outputId": "87f0ba78-d66f-4615-c7e2-f9b0a920e2d2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Headlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>a  hindrance to operations   extracts from the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>scorecard the best lake scene leader  german s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>coventry caught on counter by flo united s riv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>pilgrim knows how to progress thatcher facing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>hitches and horlocks beckham off but united su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>0</td>\n",
       "      <td>barclays and rbs shares suspended from trading...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>1</td>\n",
       "      <td>scientists to australia  if you want to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>1</td>\n",
       "      <td>explosion at airport in istanbul yemeni former...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>1</td>\n",
       "      <td>jamaica proposes marijuana dispensers for tour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4100</th>\n",
       "      <td>1</td>\n",
       "      <td>a     year old woman in mexico city finally re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4101 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Label                                          Headlines\n",
       "0         0  a  hindrance to operations   extracts from the...\n",
       "1         0  scorecard the best lake scene leader  german s...\n",
       "2         0  coventry caught on counter by flo united s riv...\n",
       "3         1  pilgrim knows how to progress thatcher facing ...\n",
       "4         1  hitches and horlocks beckham off but united su...\n",
       "...     ...                                                ...\n",
       "4096      0  barclays and rbs shares suspended from trading...\n",
       "4097      1        scientists to australia  if you want to ...\n",
       "4098      1  explosion at airport in istanbul yemeni former...\n",
       "4099      1  jamaica proposes marijuana dispensers for tour...\n",
       "4100      1  a     year old woman in mexico city finally re...\n",
       "\n",
       "[4101 rows x 2 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining the headlines for the rest of our records so we can convert them into vectors\n",
    "headlines = []\n",
    "for row in range(0, len(data.index)):\n",
    "  headlines.append(' '.join(str(x) for x in data.iloc[row,0:25]))\n",
    "\n",
    "\n",
    "df['Headlines'] = headlines\n",
    "\n",
    "data = df[['Label', 'Headlines']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CSk4ZqWHDik8",
    "outputId": "c7b16aeb-c225-4379-f594-bddd022dcc09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4101\n",
      "4101\n",
      "3075\n",
      "3075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set our X and y\n",
    "# X -> features\n",
    "# y -> target\n",
    "\n",
    "X = data['Headlines']\n",
    "y = data[\"Label\"]\n",
    "\n",
    "print(len(data))\n",
    "print(len(y))\n",
    "\n",
    "# Perform our train test split using sklearn's train_test_split function\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFut9la6G2zw"
   },
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "Using 'bag of words' model and a TF-IDF Vectorizer for converting text data into vectors. \n",
    "\n",
    "For both Random Forest Classifier & Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "I105beVWqPQw"
   },
   "outputs": [],
   "source": [
    "# Count vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "m7nxIGEZqaJo"
   },
   "outputs": [],
   "source": [
    "# Implementing Bag of Words model: \n",
    "countvector = CountVectorizer(ngram_range=(2, 2))\n",
    "train_dataset = countvector.fit_transform(X_train) # Converting all the headlines to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "grL_ZJI9qsgo",
    "outputId": "0ee46b2e-e7cb-49f2-c0a4-85a5c310d382"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x477679 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 138 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data converts into sparse matrix \n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyCfiHO4qx-o"
   },
   "source": [
    "First, we'll use our bag of words model with Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HSa8_-KLq0is",
    "outputId": "9f3607cb-87a4-4d43-92f0-eae45a5c2bd9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement random forest classifier\n",
    "random_classifier = RandomForestClassifier(n_estimators=200, criterion='entropy')\n",
    "random_classifier.fit(train_dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "35I69E0IzWnI"
   },
   "outputs": [],
   "source": [
    "# Predicting for our test dataset\n",
    "test_dataset = countvector.transform(X_test)\n",
    "predictions= random_classifier.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d_qxJGm8z9rI",
    "outputId": "e57f1caf-2420-40bd-e3e1-096401a3bb6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "8D4CaOmN0jZM"
   },
   "outputs": [],
   "source": [
    "# For calculating our accuracy\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpS_3gRb1fsk",
    "outputId": "299512ad-af9b-4c4d-9599-039929dad895"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20 466]\n",
      " [ 37 503]]\n",
      "0.5097465886939572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.04      0.07       486\n",
      "           1       0.52      0.93      0.67       540\n",
      "\n",
      "    accuracy                           0.51      1026\n",
      "   macro avg       0.43      0.49      0.37      1026\n",
      "weighted avg       0.44      0.51      0.39      1026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix= confusion_matrix(y_test,predictions)\n",
    "print(matrix)\n",
    "score= accuracy_score(y_test,predictions)\n",
    "print(score)\n",
    "report= classification_report(y_test,predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IzMZISl195Y"
   },
   "source": [
    "It looks like our overall accuracy is around 51% with this approach. \n",
    "\n",
    "The notebook I based my initial preprocessing and approach off (cited below, Sun, J.) acheived a nearly 84% accuracy with this approach, although I've split my test and train data somewhat differently. \n",
    "\n",
    "In Sun, J.'s approach, the test and train data actually overlapped quite a bit: for the entire year 2015. My concern is with having a model overfit in our production case, and so I opted to use the SciKit Learn's built in train_test_split function. \n",
    "\n",
    "I'm not yet prepared to say whether our low accuracy is due to our preprocessing methods or something else, so I'm going to continue exploration prior to using our actual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQRb8-rc2ha0"
   },
   "source": [
    "# Using Random Forest Classfier with TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "lWU73bwm2gIA"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "VcFEBOpf2yp8"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(2, 2))\n",
    "train_dataset= tfidf.fit_transform(X_train) # Changing all our headlines to vectors using TF-IDF technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqQembFA3H64",
    "outputId": "44f9649c-abdc-4196-8380-614ab496d912"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='entropy', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement random forest classfier on train_dataset\n",
    "random_classifier = RandomForestClassifier(n_estimators=200, criterion='entropy')\n",
    "random_classifier.fit(train_dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "welCIZCI3grQ"
   },
   "outputs": [],
   "source": [
    "# Predict on test dataset\n",
    "test_dataset= tfidf.transform(X_test)\n",
    "predictions= random_classifier.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwk8-fTW3-Sk",
    "outputId": "7631e086-41e2-42f1-8f2b-be24bb169579"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQg7A0dQ4BqM",
    "outputId": "03e55c75-27b6-4717-fd91-bc620f2469bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100 386]\n",
      " [124 416]]\n",
      "0.5029239766081871\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.21      0.28       486\n",
      "           1       0.52      0.77      0.62       540\n",
      "\n",
      "    accuracy                           0.50      1026\n",
      "   macro avg       0.48      0.49      0.45      1026\n",
      "weighted avg       0.48      0.50      0.46      1026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ACCURACY AFTER USING TF-IDF VECTORIZER\n",
    "matrix= confusion_matrix(y_test,predictions)\n",
    "print(matrix)\n",
    "score= accuracy_score(y_test,predictions)\n",
    "print(score)\n",
    "report= classification_report(y_test,predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8iR0t_q56EM"
   },
   "source": [
    "It still looks like our accuracy is quite low, and further optimizations would  be required with these approaches.\n",
    "\n",
    "However, first, we're going to try our Naive-Bayes Classifier and see how that performs, and then we're going to examine another architecture that may be better for this particular sentiment analysis problem: an LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjcBqzng8oNg"
   },
   "source": [
    "## Naive-Bayes Classifier\n",
    "\n",
    "First, we'll use our 'bag of words' model to convert text into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "v2Qb9z5D5ltE"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "H4giyMcl8-c8"
   },
   "outputs": [],
   "source": [
    "countvector = CountVectorizer(ngram_range=(2, 2))\n",
    "train_dataset = countvector.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lx6n-g3A9e38",
    "outputId": "eaa3d471-e351-46e7-e681-9d5c1fe945e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 96,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Naive Bayes Classifier with training data\n",
    "naive.fit(train_dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "y7rpjdxb9tOs"
   },
   "outputs": [],
   "source": [
    "# Predicting on test dataset\n",
    "test_dataset = countvector.transform(X_test)\n",
    "predictions = naive.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g8Mg6zoy-IpY",
    "outputId": "3dd5c9fc-4a1b-4cc4-f18a-07ead01a4519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3nrx_OY5-MPE",
    "outputId": "33d6f6a8-af5f-4523-b912-1ee211e8d7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[131 355]\n",
      " [143 397]]\n",
      "0.5146198830409356\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.27      0.34       486\n",
      "           1       0.53      0.74      0.61       540\n",
      "\n",
      "    accuracy                           0.51      1026\n",
      "   macro avg       0.50      0.50      0.48      1026\n",
      "weighted avg       0.50      0.51      0.49      1026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix= confusion_matrix(y_test, predictions)\n",
    "print(matrix)\n",
    "score= accuracy_score(y_test, predictions)\n",
    "print(score)\n",
    "report= classification_report(y_test, predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXUII6ZG-eJ8"
   },
   "source": [
    "We could try Naive-Bayes with TF-IDF Vectorization, but I'm not convinced that it's necessarily going to perform any better for this problem. I'd like to take a different approach. First, I want to look at LSTM RNN's, and see how one performs on this dataset.\n",
    "\n",
    "Knowing that LSTM's often need a LOT of data to perform well, I'm then going to see if I can combine the data that I have, preprocess it accordingly, and provide the LSTM with the combined set so it's better able to train. \n",
    "\n",
    "First, though, I'm going to just 'prove concept' with the current dataset that we're working with, and see if our accuracy even marginally improves. It's possible another type of architecture altogether suits us better, so let's see!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZ-n_a6GpWpU"
   },
   "source": [
    "# LSTM with bag of words model & limited \"trial\" data\n",
    "\n",
    "LABELS: \n",
    "\n",
    "0 -> Stock will go down\n",
    "\n",
    "1 -> Stock will go up or hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Sm9iBl_9-2U0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.9/site-packages (2.4.3)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.9/site-packages (from keras) (3.2.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.9/site-packages (from keras) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.9/site-packages (from keras) (1.6.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/site-packages (from keras) (5.4.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0fdd38bb221d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install tensorflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "!pip3 install keras\n",
    "!pip3 install tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aQ1QMIO2GY0g"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-75a531a0dec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "\n",
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X = tokenizer.texts_to_sequences(X_train)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jK5Qkn05HHNQ"
   },
   "outputs": [],
   "source": [
    "# Compose the LSTM network.\n",
    "# embed_dim, lstm_out, batch_size, and dropout_x are hyperparameters that may need to be tweaked in order to optimize output\n",
    "# input_length, which we pass to our Embedding layer, takes the 1st index of our X value -> or, the length of our features?\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length = X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNbvoHPgIGg0"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "print(type(X))\n",
    "print(type(y_train))\n",
    "model.fit(X, y_train, epochs = 15, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e90pAcUpUU4"
   },
   "source": [
    "# LSTM with Tokenization and final dataset\n",
    "\n",
    "### Using by-company stock data scraped from Reddit & Yahoo Finance\n",
    "\n",
    "Using what we've learned from our initial approaches, we will be creating our final model on our actual data, stepping from a basic \"market will go up or down\" approach to a more specific \"Company X's stock will go up or down based on relevant news about Company X\". \n",
    "\n",
    "LABELS: \n",
    "\n",
    "0 -> Stock will go down (negative)\n",
    "\n",
    "1 -> Stock will go up (positive)\n",
    "\n",
    "2 -> Stock will hold (neutral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910
    },
    "id": "oiig_oSHpmcQ",
    "outputId": "72665a68-b353-4ddc-c770-3080079fbbf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Label Ticker                                           Headline\n",
      "0         0    MMM  Employer who stole nearly $3M in wages from 15...\n",
      "1         1    MMM  Huge new Facebook data leak exposed intimate d...\n",
      "2         0    MMM  A campaign has accelerated to turn a disused r...\n",
      "3         1    MMM  Google launches global human trafficking helpl...\n",
      "4         1    MMM  Over 3m Saudi Women Dont Have ID Cards; Saudi...\n",
      "...     ...    ...                                                ...\n",
      "2376      0    WMT  Walmart dumps e-cigarettes: Largest store in U...\n",
      "2377      0    WMT  Walmart makes a $16 billion bet on India's boo...\n",
      "2378      0    WMT  Walmart raises minimum age to buy tobacco to 2...\n",
      "2379      0    WMT  Walmart Took Over Chile In Only Three Years An...\n",
      "2380      2    WMT  Carla Cheney: Walmart Fired Me For Reporting D...\n",
      "\n",
      "[2381 rows x 3 columns]\n",
      "       Label Ticker                                           Headline\n",
      "0          0      A  @TotesTravel : Airline shares tumble as New Yo...\n",
      "1          1      A  @TotesTravel : American United call off Hong K...\n",
      "2          0      A  @TotesTravel : U.S. airline stocks hit highest...\n",
      "3          1      A  @TotesTravel : American Airlines reaches deal ...\n",
      "4          1      A  @TotesTravel : US airlines Treasury Department...\n",
      "...      ...    ...                                                ...\n",
      "13176      1   ZNGA  Bitcoin Tops $1000 Again as Zynga Accepts Virt...\n",
      "13177      1   ZNGA        Zynga Accepts Bitcoin For Microtransactions\n",
      "13178      1   ZUMZ  Zumiez (ZUMZ) unusual put activity into earnin...\n",
      "13179      1   ZUMZ                           Zumiez Is Going Bankrupt\n",
      "13180      1   ZUMZ                   Zumiez Is Going Bankrupt!! Omfg!\n",
      "\n",
      "[13181 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>Employer who stole nearly $3M in wages from 15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MMM</td>\n",
       "      <td>Huge new Facebook data leak exposed intimate d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>A campaign has accelerated to turn a disused r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>MMM</td>\n",
       "      <td>Google launches global human trafficking helpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>MMM</td>\n",
       "      <td>Over 3m Saudi Women Dont Have ID Cards; Saudi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15557</th>\n",
       "      <td>1</td>\n",
       "      <td>ZNGA</td>\n",
       "      <td>Bitcoin Tops $1000 Again as Zynga Accepts Virt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15558</th>\n",
       "      <td>1</td>\n",
       "      <td>ZNGA</td>\n",
       "      <td>Zynga Accepts Bitcoin For Microtransactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15559</th>\n",
       "      <td>1</td>\n",
       "      <td>ZUMZ</td>\n",
       "      <td>Zumiez (ZUMZ) unusual put activity into earnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15560</th>\n",
       "      <td>1</td>\n",
       "      <td>ZUMZ</td>\n",
       "      <td>Zumiez Is Going Bankrupt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15561</th>\n",
       "      <td>1</td>\n",
       "      <td>ZUMZ</td>\n",
       "      <td>Zumiez Is Going Bankrupt!! Omfg!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15562 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label Ticker                                           Headline\n",
       "0          0    MMM  Employer who stole nearly $3M in wages from 15...\n",
       "1          1    MMM  Huge new Facebook data leak exposed intimate d...\n",
       "2          0    MMM  A campaign has accelerated to turn a disused r...\n",
       "3          1    MMM  Google launches global human trafficking helpl...\n",
       "4          1    MMM  Over 3m Saudi Women Dont Have ID Cards; Saudi...\n",
       "...      ...    ...                                                ...\n",
       "15557      1   ZNGA  Bitcoin Tops $1000 Again as Zynga Accepts Virt...\n",
       "15558      1   ZNGA        Zynga Accepts Bitcoin For Microtransactions\n",
       "15559      1   ZUMZ  Zumiez (ZUMZ) unusual put activity into earnin...\n",
       "15560      1   ZUMZ                           Zumiez Is Going Bankrupt\n",
       "15561      1   ZUMZ                   Zumiez Is Going Bankrupt!! Omfg!\n",
       "\n",
       "[15562 rows x 3 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data & perform sentiment analysis to provide training labels\n",
    "djia_df = pd.read_csv(\"/content/drive/MyDrive/StockStalker/djia_news.csv\")\n",
    "nasdaq_df = pd.read_csv('/content/drive/MyDrive/StockStalker/nasdaq.csv')\n",
    "\n",
    "print(djia_df)\n",
    "print(nasdaq_df)\n",
    "\n",
    "df = pd.concat([djia_df, nasdaq_df], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "DKrISjnjhwvo",
    "outputId": "c7e7094a-bccb-4fb2-f5c8-00e06430c1f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Headline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>MMM employer who stole nearly   m in wages fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MMM</td>\n",
       "      <td>MMM huge new facebook data leak exposed intima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>MMM a campaign has accelerated to turn a disus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>MMM</td>\n",
       "      <td>MMM google launches global human trafficking h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>MMM</td>\n",
       "      <td>MMM over  m saudi women don t have id cards  s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>MMM</td>\n",
       "      <td>MMM boris johnson promises tax cut for  m high...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>MMM canada spends   m to stop female genital m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>MMM</td>\n",
       "      <td>MMM u s  accuses china of detaining up to  m x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>MMM more than   m raised for humboldt broncos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>MMM</td>\n",
       "      <td>MMM australian property giant meriton has been...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label Ticker                                           Headline\n",
       "0      0    MMM  MMM employer who stole nearly   m in wages fro...\n",
       "1      1    MMM  MMM huge new facebook data leak exposed intima...\n",
       "2      0    MMM  MMM a campaign has accelerated to turn a disus...\n",
       "3      1    MMM  MMM google launches global human trafficking h...\n",
       "4      1    MMM  MMM over  m saudi women don t have id cards  s...\n",
       "5      1    MMM  MMM boris johnson promises tax cut for  m high...\n",
       "6      0    MMM  MMM canada spends   m to stop female genital m...\n",
       "7      1    MMM  MMM u s  accuses china of detaining up to  m x...\n",
       "8      0    MMM  MMM more than   m raised for humboldt broncos ...\n",
       "9      0    MMM  MMM australian property giant meriton has been..."
      ]
     },
     "execution_count": 102,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "df.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n",
    "\n",
    "df[\"Headline\"] = df[\"Headline\"].str.lower()\n",
    "\n",
    "df[\"Headline\"] = df[\"Ticker\"] + ' ' + df[\"Headline\"]\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAE_II_ntwEA",
    "outputId": "154916e1-17dc-4cdd-ceda-6d5a732cdc95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode the labels\n",
    "from keras.utils.np_utils import to_categorical\n",
    "labels = to_categorical(df[\"Label\"], num_classes=3)\n",
    "\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEmy74idl0_4",
    "outputId": "67b3bec9-a056-45e6-b948-69752e0cc034"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11671, 2)\n",
      "(11671, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split our train and test data\n",
    "\n",
    "X = df.drop('Label', axis=1)\n",
    "y = df['Label']\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DkNYT-mDnSk8",
    "outputId": "e940e43d-1a6e-4c1a-b097-a8ece5dbece2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer: <keras_preprocessing.text.Tokenizer object at 0x7f15a887fe10>\n",
      "X size after to_sequences: 11671\n",
      "(11671, 66)\n",
      "(11671, 3)\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "\n",
    "max_features = 8000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "print(f\"tokenizer: {tokenizer}\")\n",
    "tokenizer.fit_on_texts(X_train['Headline'])\n",
    "X = tokenizer.texts_to_sequences(X_train['Headline'])\n",
    "print(f\"X size after to_sequences: {len(X)}\")\n",
    "\n",
    "X = pad_sequences(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpiWskl5AVqs"
   },
   "source": [
    "We want to optimize our LSTM network, so we're going to compose our model within a get_model() function that we can use with a library called talos. \n",
    "\n",
    "Talos enables us to automate hyperparameter tuning very easily with the keras Sequential API. \n",
    "\n",
    "With this method, we're looking to find the best parameters for our model to get the highest accuracy on our validation and test sets (and hopefully, on real world examples when this model enters production)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "MjNPvvVEngXw"
   },
   "outputs": [],
   "source": [
    "# Compose the LSTM network.\n",
    "\n",
    "# Create get_model function to be able to implement Bayesian optimization using talos\n",
    "# !pip3 install talos\n",
    "import talos as ta\n",
    "\n",
    "def get_model(X_train, y_train, x_val, y_val, params):\n",
    "  \"\"\"\n",
    "  Define LSTM model to predict stock changes.\n",
    "\n",
    "  Args:\n",
    "    input: X value to determine Embedding layer input_length\n",
    "    dropout: LSTM dropout & recurrent_dropout rate (Float)\n",
    "\n",
    "  Returns:\n",
    "    a Keras model.\n",
    "  \"\"\"\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_features, params['embed_dim'], input_length=X_train.shape[1]))\n",
    "  model.add(SpatialDropout1D(0.4))\n",
    "  model.add(LSTM(params['lstm_out'], dropout=params['dropout'], recurrent_dropout=params['dropout']))\n",
    "  model.add(Dense(3, activation='softmax'))\n",
    "  model.compile(loss = 'categorical_crossentropy', optimizer=params['optimizer'], metrics = ['accuracy'])\n",
    "  print(model.summary())\n",
    "  history = model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], validation_split=0.3)\n",
    "  return history, model\n",
    "\n",
    "p = {\n",
    "    'embed_dim': [128, 150, 200],\n",
    "    'lstm_out': [196, 200, 204],\n",
    "    'batch_size': (32, 28, 12),\n",
    "    'optimizer': [\"adam\", \"nadam\"],\n",
    "    'dropout': (0, 0.5, 0.2, 0.5),\n",
    "    'epochs': [10]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZDuU91Rnn14"
   },
   "outputs": [],
   "source": [
    "# Our first round of training with our model architecture on our dataset\n",
    "# Our model looks somewhat overfit - with our test accuracy up to 99% and our val accuracy staying around 66%\n",
    "# Above, we've refactored our model setup into a function, and are using talos scanning to optimize performance\n",
    "batch_size = 32\n",
    "print(type(X))\n",
    "print(type(y_train))\n",
    "# model.fit(X, y_train, epochs = 100, batch_size=batch_size, verbose = 2, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oy8K1UX66sus"
   },
   "outputs": [],
   "source": [
    "# and run the first experiment\n",
    "t= ta.Scan(X, y_train, p, get_model, \"exp_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VwAWkQqb0PH4",
    "outputId": "99d6cd7d-7517-4d0f-8d06-e51f76455c07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 66, 128)           1024000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 66, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 591       \n",
      "=================================================================\n",
      "Total params: 1,279,391\n",
      "Trainable params: 1,279,391\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "511/511 [==============================] - 169s 326ms/step - loss: 0.7601 - accuracy: 0.6375 - val_loss: 0.7344 - val_accuracy: 0.6259\n",
      "Epoch 2/20\n",
      "511/511 [==============================] - 166s 325ms/step - loss: 0.6096 - accuracy: 0.7091 - val_loss: 0.6947 - val_accuracy: 0.6451\n",
      "Epoch 3/20\n",
      "511/511 [==============================] - 166s 324ms/step - loss: 0.4154 - accuracy: 0.8233 - val_loss: 0.7336 - val_accuracy: 0.6582\n",
      "Epoch 4/20\n",
      "511/511 [==============================] - 166s 324ms/step - loss: 0.2962 - accuracy: 0.8852 - val_loss: 0.8577 - val_accuracy: 0.6639\n",
      "Epoch 5/20\n",
      "511/511 [==============================] - 165s 323ms/step - loss: 0.2079 - accuracy: 0.9241 - val_loss: 0.9043 - val_accuracy: 0.6553\n",
      "Epoch 6/20\n",
      "511/511 [==============================] - 166s 325ms/step - loss: 0.1613 - accuracy: 0.9439 - val_loss: 1.0542 - val_accuracy: 0.6676\n",
      "Epoch 7/20\n",
      "511/511 [==============================] - 165s 323ms/step - loss: 0.1247 - accuracy: 0.9574 - val_loss: 1.1657 - val_accuracy: 0.6765\n",
      "Epoch 8/20\n",
      "511/511 [==============================] - 166s 324ms/step - loss: 0.1085 - accuracy: 0.9643 - val_loss: 1.2837 - val_accuracy: 0.6708\n",
      "Epoch 9/20\n",
      "511/511 [==============================] - 165s 324ms/step - loss: 0.0975 - accuracy: 0.9640 - val_loss: 1.2940 - val_accuracy: 0.6745\n",
      "Epoch 10/20\n",
      "511/511 [==============================] - 165s 324ms/step - loss: 0.0774 - accuracy: 0.9749 - val_loss: 1.4687 - val_accuracy: 0.6719\n",
      "Epoch 11/20\n",
      "511/511 [==============================] - 165s 324ms/step - loss: 0.0732 - accuracy: 0.9698 - val_loss: 1.5621 - val_accuracy: 0.6853\n",
      "Epoch 12/20\n",
      "511/511 [==============================] - 165s 323ms/step - loss: 0.0613 - accuracy: 0.9734 - val_loss: 1.4024 - val_accuracy: 0.6725\n",
      "Epoch 13/20\n",
      "511/511 [==============================] - 165s 324ms/step - loss: 0.0613 - accuracy: 0.9730 - val_loss: 1.6078 - val_accuracy: 0.6719\n",
      "Epoch 14/20\n",
      "511/511 [==============================] - 165s 323ms/step - loss: 0.0477 - accuracy: 0.9790 - val_loss: 1.7261 - val_accuracy: 0.6739\n",
      "Epoch 15/20\n",
      "511/511 [==============================] - 166s 324ms/step - loss: 0.0459 - accuracy: 0.9817 - val_loss: 1.8453 - val_accuracy: 0.6770\n",
      "Epoch 16/20\n",
      "511/511 [==============================] - 165s 324ms/step - loss: 0.0462 - accuracy: 0.9792 - val_loss: 1.7657 - val_accuracy: 0.6728\n",
      "Epoch 17/20\n",
      "511/511 [==============================] - 166s 324ms/step - loss: 0.0492 - accuracy: 0.9792 - val_loss: 2.0102 - val_accuracy: 0.6759\n",
      "Epoch 18/20\n",
      "511/511 [==============================] - 166s 324ms/step - loss: 0.0376 - accuracy: 0.9843 - val_loss: 1.9026 - val_accuracy: 0.6645\n",
      "Epoch 19/20\n",
      "511/511 [==============================] - 166s 325ms/step - loss: 0.0390 - accuracy: 0.9820 - val_loss: 1.9179 - val_accuracy: 0.6685\n",
      "Epoch 20/20\n",
      "511/511 [==============================] - 165s 323ms/step - loss: 0.0412 - accuracy: 0.9788 - val_loss: 1.9719 - val_accuracy: 0.6773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1549d671d0>"
      ]
     },
     "execution_count": 112,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer=\"adam\", metrics = ['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X, y_train, epochs=20, batch_size=16, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzlZY9k1l36Y",
    "outputId": "439a2f6a-4adc-4d6d-c8d2-6a18ebb8b0d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size after to_sequences: 3891\n",
      "y_test size: (3891, 3)\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 66) for input KerasTensor(type_spec=TensorSpec(shape=(None, 66), dtype=tf.float32, name='embedding_2_input'), name='embedding_2_input', description=\"created by layer 'embedding_2_input'\"), but it was called on an input with incompatible shape (None, 65).\n",
      "122/122 [==============================] - 5s 38ms/step - loss: 2.5593 - accuracy: 0.5533\n"
     ]
    }
   ],
   "source": [
    "# We haven't tokenized our test data yet, so we'll do that and then evaluate our trained model's performance\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(X_test['Headline'])\n",
    "X = tokenizer.texts_to_sequences(X_test['Headline'])\n",
    "print(f\"X size after to_sequences: {len(X)}\")\n",
    "print(f\"y_test size: {y_test.shape}\")\n",
    "\n",
    "X = pad_sequences(X)\n",
    "\n",
    "accr = model.evaluate(X, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWgiUsV8AXS4",
    "outputId": "786eb3d3-7848-4c1d-fa6b-bf48c65b714c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.0169281e-01 9.8235704e-02 7.1436916e-05]] Stock will decrease\n"
     ]
    }
   ],
   "source": [
    "# Call predict on a test 'headline' and check our output\n",
    "test_headline = ['aapl apple criticized for data practices']\n",
    "\n",
    "seq = tokenizer.texts_to_sequences(test_headline)\n",
    "padded = pad_sequences(seq)\n",
    "pred = model.predict(padded)\n",
    "labels = ['Stock will decrease', 'Stock will increase', 'Stock will hold']\n",
    "print(pred, labels[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCV6SCDfBiR0",
    "outputId": "accb621c-6283-4c2c-c9fe-59c4ca82f269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/sidneyarcidiacono/Projects/stockstalker/predict/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('/Users/sidneyarcidiacono/Projects/stockstalker/predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95l8YaP0hUqE"
   },
   "source": [
    "Notes from office hours: \n",
    "\n",
    "Grid search, random search, bayesian optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a1UvY5dWA4Hk"
   },
   "source": [
    "Citations:\n",
    "\n",
    "Sun, J. (2016, August). Daily News for Stock Market Prediction, Version 1. Retrieved (2021, March) from https://www.kaggle.com/aaron7sun/stocknews.\n",
    "\n",
    "Autonomio Talos [Computer software]. (2019). Retrieved from http://github.com/autonomio/talos."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "StockStalker.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
